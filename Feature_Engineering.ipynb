{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Packages**"
      ],
      "metadata": {
        "id": "t5wewbNqz8cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpP642wc8kXg",
        "outputId": "320d6b5f-f0c8-4b82-9e40-7553f22307e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat) (0.13.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language-tool-python) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language-tool-python) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (2022.9.24)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n",
        "!pip install sentencepiece  \n",
        "!pip install transformers\n",
        "!pip install textstat\n",
        "!pip install language-tool-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart Runtime after installing "
      ],
      "metadata": {
        "id": "oFW9msHtUg-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Packages**"
      ],
      "metadata": {
        "id": "-CLigcFJ0ABg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzNPxq8X8bYG",
        "outputId": "1d5da78d-9e23-4c8b-e451-18266c9295a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Downloading LanguageTool 5.7: 100%|██████████| 225M/225M [00:12<00:00, 17.8MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmp9940ntp8.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-5.7.zip to /root/.cache/language_tool_python.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk \n",
        "import spacy\n",
        "import textstat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn \n",
        "from textblob import Word\n",
        "import matplotlib.pyplot as plt\n",
        "import language_tool_python\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tool = language_tool_python.LanguageTool('en-US')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading Data From Google Drive**"
      ],
      "metadata": {
        "id": "B9QmvZ2KGsPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7JhmdSg8swA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "6023a128-9a55-4fcb-ac1a-b1905a3e108a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                              Essay  Rater_1 Score  \\\n",
              "0   1  Dear local newspaper, I think effects computer...            4.0   \n",
              "1   2  Dear @CAPS1 @CAPS2, I believe that using compu...            5.0   \n",
              "2   3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...            4.0   \n",
              "3   4  Dear Local Newspaper, @CAPS1 I have found that...            5.0   \n",
              "4   5  Dear @LOCATION1, I know having computers has a...            4.0   \n",
              "\n",
              "   Rater_2 Score  Total Score  \n",
              "0            4.0          8.0  \n",
              "1            4.0          9.0  \n",
              "2            3.0          7.0  \n",
              "3            5.0         10.0  \n",
              "4            4.0          8.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28ef0aa9-2c03-485a-81b7-6bde2d7bee05\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28ef0aa9-2c03-485a-81b7-6bde2d7bee05')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-28ef0aa9-2c03-485a-81b7-6bde2d7bee05 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-28ef0aa9-2c03-485a-81b7-6bde2d7bee05');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "Data_Essay_01 = pd.read_csv(\"/content/drive/MyDrive/IntelliTech-DataSet/EssaySet01.csv\")\n",
        "Data_Essay_01.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4CyChIa3hm3"
      },
      "source": [
        "# **Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Essay Pre Processing**"
      ],
      "metadata": {
        "id": "0d9vjiulC7aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Remove_NER(Essay):\n",
        "  \"\"\"\n",
        "    Removes Named Entity Recognition (NER) from each essay\n",
        "\n",
        "    Args:\n",
        "      Sentence: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "\n",
        "  \"\"\"\n",
        "  return ' '.join (word for word in Essay.split(' ') if not word.startswith('@'))\n",
        "\n",
        "def Remove_Punctuations(sentence):\n",
        "  \"\"\"\n",
        "    Removes punctuations from text\n",
        "    Args:\n",
        "      sentence: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  punctuations = '''!()-[]{};:\"\\,/'<>.?@#$%^&*_~'''\n",
        "  newSentence = \"\"\n",
        "  for word in sentence:\n",
        "      if (word in punctuations):\n",
        "          newSentence = newSentence + \" \"\n",
        "      else: \n",
        "          newSentence = newSentence + word\n",
        "  return newSentence\n",
        "\n",
        "def LowerCase_Words(Essay):\n",
        "  \"\"\"\n",
        "    Lower case all the words in an essay\n",
        "\n",
        "    Args:\n",
        "      Sentence: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  return re.sub('[0-9]+','', Essay).lower() \n",
        "\n",
        "def Tokenize_Essay(Essay):\n",
        "    \"\"\"\n",
        "      Create Tokens of each Essay\n",
        "\n",
        "      Args:\n",
        "        Essay: Essay of each student\n",
        "      \n",
        "      Returns: \n",
        "        String\n",
        "    \"\"\"\n",
        "    Preprocessed = Remove_Punctuations(Essay)\n",
        "    return \" \".join(word_tokenize(Preprocessed))\n",
        "\n",
        "def Remove_White_Spaces(Essay):\n",
        "  \"\"\"\n",
        "    Removes Extra White Spaces\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  return \" \".join(Essay.split())\n",
        "\n",
        "def Remove_Special_Characters(Essay):\n",
        "  \"\"\"\n",
        "    Removes Special Characters from Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  new_text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", Essay)\n",
        "  return new_text"
      ],
      "metadata": {
        "id": "6qLIKn4LC7-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Basic Count Features**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Sentences per Essay\n",
        "*   Counting Words per Essay\n",
        "*   Counting Characters per Essay\n",
        "*   Average Words per Essay\n",
        "*   Counting Syllables\n"
      ],
      "metadata": {
        "id": "pmuQWvabiYC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Sentences per Essay"
      ],
      "metadata": {
        "id": "9jYVbje0Ex_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV3ttAkblI57"
      },
      "outputs": [],
      "source": [
        "def Sentence_Count(Essay):\n",
        "    \"\"\"\n",
        "    Counts sentences in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "    sentence_no = nltk.sent_tokenize(Essay)\n",
        "    return len(sentence_no)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Sent_Count'] = Data_Essay_01['Essay'].apply(Sentence_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "hQzk8MtddUbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d607f1-f903-49d3-c99a-f9421caf25d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ID                                              Essay  Rater_1 Score  \\\n",
              "1053  1056  Many people say that computer is help them in ...            3.0   \n",
              "\n",
              "      Rater_2 Score  Total Score  Sent_Count  \n",
              "1053            3.0          6.0          18  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ad6a3a8-eb93-4251-915c-20e3b0f0432f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>1056</td>\n",
              "      <td>Many people say that computer is help them in ...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ad6a3a8-eb93-4251-915c-20e3b0f0432f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ad6a3a8-eb93-4251-915c-20e3b0f0432f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ad6a3a8-eb93-4251-915c-20e3b0f0432f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Words per Essay"
      ],
      "metadata": {
        "id": "6VY92aM5E00E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** These word count are more than the original count coz of nltk tokenization. Punctations are treated as seperate words.\n"
      ],
      "metadata": {
        "id": "YxGrgzWIFzPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Word_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts words in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int  \n",
        "  \"\"\" \n",
        "  word_no = nltk.word_tokenize(Essay)\n",
        "  return len(word_no)"
      ],
      "metadata": {
        "id": "7OPVrqpAdLec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Word_Count'] = Data_Essay_01['Essay'].apply(Word_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "8kb3-p5edQzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdf1ef7-139d-4b4c-b38e-6a4688430b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID                                              Essay  Rater_1 Score  \\\n",
              "585  588  Dear the newspaper, Computers are great resour...            5.0   \n",
              "\n",
              "     Rater_2 Score  Total Score  Sent_Count  Word_Count  \n",
              "585            5.0         10.0          32         601  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c514ba0-37e3-4e40-b0f8-508c813f0b32\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Word_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>585</th>\n",
              "      <td>588</td>\n",
              "      <td>Dear the newspaper, Computers are great resour...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>32</td>\n",
              "      <td>601</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c514ba0-37e3-4e40-b0f8-508c813f0b32')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8c514ba0-37e3-4e40-b0f8-508c813f0b32 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8c514ba0-37e3-4e40-b0f8-508c813f0b32');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Characters per Essay"
      ],
      "metadata": {
        "id": "LCGMHv_yE29j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Char_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts characters in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "  return len([character for character in Essay])"
      ],
      "metadata": {
        "id": "ELmS3Tt-f-nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Char_Count'] = Data_Essay_01['Essay'].apply(Char_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "AchO9qj9dMzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12a92e7-5862-4e34-aa0f-fa1c4d77eef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID                                              Essay  Rater_1 Score  \\\n",
              "317  319  Computers definitely have a positive impact on...            4.0   \n",
              "\n",
              "     Rater_2 Score  Total Score  Sent_Count  Word_Count  Char_Count  \n",
              "317            5.0          9.0          29         449        2171  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8201a8f9-8d9c-4384-9c9e-dd02c26235b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Word_Count</th>\n",
              "      <th>Char_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>319</td>\n",
              "      <td>Computers definitely have a positive impact on...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>29</td>\n",
              "      <td>449</td>\n",
              "      <td>2171</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8201a8f9-8d9c-4384-9c9e-dd02c26235b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8201a8f9-8d9c-4384-9c9e-dd02c26235b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8201a8f9-8d9c-4384-9c9e-dd02c26235b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Average Word Length of Essay"
      ],
      "metadata": {
        "id": "eSWTBDDpizP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Avg_Word_Count(Essay):\n",
        "  \"\"\"\n",
        "    Calculates Average Word Count In An Essay Set\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      float\n",
        "      \n",
        "  \"\"\"\n",
        "  word_list = nltk.word_tokenize(Essay)\n",
        "  total = sum(map(len, word_list))/len(word_list)\n",
        "  return total"
      ],
      "metadata": {
        "id": "HujBMTSuXxxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Avg_Word_Count'] = Data_Essay_01['Essay'].apply(Avg_Word_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "D5A2-UkIdX4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4f22f7-31a5-44c8-e161-7acb776db90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    ID                                              Essay  Rater_1 Score  \\\n",
              "32  33  Dear, @ORGANIZATION1 I think the effects that ...            3.0   \n",
              "\n",
              "    Rater_2 Score  Total Score  Sent_Count  Word_Count  Char_Count  \\\n",
              "32            3.0          6.0          12         191         917   \n",
              "\n",
              "    Avg_Word_Count  \n",
              "32        3.931937  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d4ae225-9767-4b0b-8069-7b60f12a5616\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Word_Count</th>\n",
              "      <th>Char_Count</th>\n",
              "      <th>Avg_Word_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>Dear, @ORGANIZATION1 I think the effects that ...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>12</td>\n",
              "      <td>191</td>\n",
              "      <td>917</td>\n",
              "      <td>3.931937</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d4ae225-9767-4b0b-8069-7b60f12a5616')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d4ae225-9767-4b0b-8069-7b60f12a5616 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d4ae225-9767-4b0b-8069-7b60f12a5616');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Syllables"
      ],
      "metadata": {
        "id": "SSNhc_FfKHld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Syllable_Count(text):\n",
        "  return textstat.syllable_count(text, lang='en_US')"
      ],
      "metadata": {
        "id": "1ihBnNT6KGnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parts Of Speech Counts**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Nouns per Essay\n",
        "*   Counting Adjectives per Essay\n",
        "*   Counting Proper Nouns per Essay\n",
        "*   Counting Adverbs per Essay\n",
        "*   Counting Conjunctions per Essay"
      ],
      "metadata": {
        "id": "iUMnW4Qm4Vws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing NERs, Punctuations and Lower Casing"
      ],
      "metadata": {
        "id": "A0qjHkwRH7FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Preprocessed_Essay'] = Data_Essay_01['Essay'].apply(Remove_NER)\n",
        "Data_Essay_01['Preprocessed_Essay'] = Data_Essay_01['Preprocessed_Essay'].apply(Tokenize_Essay)\n",
        "Data_Essay_01.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "mxsK-yprDcfV",
        "outputId": "e3565749-f142-4a4f-e828-b7e419b1e592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                              Essay  Rater_1 Score  \\\n",
              "0   1  Dear local newspaper, I think effects computer...            4.0   \n",
              "1   2  Dear @CAPS1 @CAPS2, I believe that using compu...            5.0   \n",
              "2   3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...            4.0   \n",
              "3   4  Dear Local Newspaper, @CAPS1 I have found that...            5.0   \n",
              "4   5  Dear @LOCATION1, I know having computers has a...            4.0   \n",
              "\n",
              "   Rater_2 Score  Total Score  Sent_Count  Word_Count  Char_Count  \\\n",
              "0            4.0          8.0          16         386        1875   \n",
              "1            4.0          9.0          20         464        2288   \n",
              "2            3.0          7.0          14         313        1541   \n",
              "3            5.0         10.0          27         611        3165   \n",
              "4            4.0          8.0          30         517        2569   \n",
              "\n",
              "   Avg_Word_Count                                 Preprocessed_Essay  \n",
              "0        3.984456  Dear local newspaper I think effects computers...  \n",
              "1        4.030172  Dear I believe that using computers will benef...  \n",
              "2        4.035144  Dear More and more people use computers but no...  \n",
              "3        4.328969  Dear Local Newspaper I have found that many ex...  \n",
              "4        4.071567  Dear I know having computers has a positive ef...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3b59356-7498-416a-af97-f59e576199f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Word_Count</th>\n",
              "      <th>Char_Count</th>\n",
              "      <th>Avg_Word_Count</th>\n",
              "      <th>Preprocessed_Essay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16</td>\n",
              "      <td>386</td>\n",
              "      <td>1875</td>\n",
              "      <td>3.984456</td>\n",
              "      <td>Dear local newspaper I think effects computers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>20</td>\n",
              "      <td>464</td>\n",
              "      <td>2288</td>\n",
              "      <td>4.030172</td>\n",
              "      <td>Dear I believe that using computers will benef...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>14</td>\n",
              "      <td>313</td>\n",
              "      <td>1541</td>\n",
              "      <td>4.035144</td>\n",
              "      <td>Dear More and more people use computers but no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>27</td>\n",
              "      <td>611</td>\n",
              "      <td>3165</td>\n",
              "      <td>4.328969</td>\n",
              "      <td>Dear Local Newspaper I have found that many ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>30</td>\n",
              "      <td>517</td>\n",
              "      <td>2569</td>\n",
              "      <td>4.071567</td>\n",
              "      <td>Dear I know having computers has a positive ef...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3b59356-7498-416a-af97-f59e576199f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f3b59356-7498-416a-af97-f59e576199f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f3b59356-7498-416a-af97-f59e576199f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Pos_Tag_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts Parts of Speech in an Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int,int,int,int,int,int    \n",
        "  \"\"\"\n",
        "  tagged_doc = nlp(Essay)\n",
        "\n",
        "  adj_count=0\n",
        "  verb_count=0\n",
        "  noun_count=0\n",
        "  pNoun_count=0\n",
        "  adverb_count=0\n",
        "  conj_count=0\n",
        "\n",
        "  for token in tagged_doc:\n",
        "\n",
        "    if(token.pos_ == 'ADJ'):\n",
        "      adj_count+=1\n",
        "    \n",
        "    elif(token.pos_ =='NOUN'):\n",
        "      noun_count+=1\n",
        "\n",
        "    elif (token.pos_ =='PRON'):\n",
        "      pNoun_count+=1\n",
        "\n",
        "    elif (token.pos_ =='VERB'):\n",
        "      verb_count+=1\n",
        "\n",
        "    elif (token.pos_ =='ADV'):\n",
        "      adverb_count+=1\n",
        "    \n",
        "    elif(token.pos_=='CCONJ'):\n",
        "      conj_count+=1\n",
        "\n",
        "  return verb_count,noun_count, adj_count, conj_count, adverb_count,pNoun_count"
      ],
      "metadata": {
        "id": "26j3wRKD4X54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Verb_Count'], Data_Essay_01['Noun_Count'], Data_Essay_01['Adj_Count'], Data_Essay_01['Conj_Count'], Data_Essay_01['Adverb_Count'], Data_Essay_01['pNoun_Count']=zip(*Data_Essay_01[\"Preprocessed_Essay\"].map(Pos_Tag_Count))\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "W-WSN5Ce85Cu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2904
        },
        "outputId": "124001e6-2eca-4d0d-c104-fc0661205a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID                                              Essay  Rater_1 Score  \\\n",
              "916  919  Dear @LOCATION1, I understand this focus situa...            5.0   \n",
              "\n",
              "     Rater_2 Score  Total Score  Sent_Count  Word_Count  Char_Count  \\\n",
              "916            6.0         11.0          22         443        2139   \n",
              "\n",
              "     Avg_Word_Count                                 Preprocessed_Essay  \\\n",
              "916        3.934537  Dear I understand this focus situation the eff...   \n",
              "\n",
              "     Verb_Count  Noun_Count  Adj_Count  Conj_Count  Adverb_Count  pNoun_Count  \n",
              "916          63          89         18           7            28           58  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7956901e-63a5-4064-a937-3e6d43aa2784\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Word_Count</th>\n",
              "      <th>Char_Count</th>\n",
              "      <th>Avg_Word_Count</th>\n",
              "      <th>Preprocessed_Essay</th>\n",
              "      <th>Verb_Count</th>\n",
              "      <th>Noun_Count</th>\n",
              "      <th>Adj_Count</th>\n",
              "      <th>Conj_Count</th>\n",
              "      <th>Adverb_Count</th>\n",
              "      <th>pNoun_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>916</th>\n",
              "      <td>919</td>\n",
              "      <td>Dear @LOCATION1, I understand this focus situa...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>22</td>\n",
              "      <td>443</td>\n",
              "      <td>2139</td>\n",
              "      <td>3.934537</td>\n",
              "      <td>Dear I understand this focus situation the eff...</td>\n",
              "      <td>63</td>\n",
              "      <td>89</td>\n",
              "      <td>18</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7956901e-63a5-4064-a937-3e6d43aa2784')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7956901e-63a5-4064-a937-3e6d43aa2784 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7956901e-63a5-4064-a937-3e6d43aa2784');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating Writing Attributes**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Style\n",
        "*   Content\n",
        "*   Semantic\n",
        "*   Semantic Coherence & Consistency \n",
        "*   Connectivity\n",
        "*   Readibility Scores\n"
      ],
      "metadata": {
        "id": "puuRk5EHw8jJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Style**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Mechanics\n",
        "*   Grammar\n",
        "*   Lexical Sophistication\n",
        "\n"
      ],
      "metadata": {
        "id": "Id4wN-9AxITZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mechanics**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Spelling Mistakes\n",
        "*   Checking Punctuations\n",
        "*   Counting Punctuations\n",
        "*   Checking Capitalization\n",
        "\n"
      ],
      "metadata": {
        "id": "I6sEA-fCCfmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Spelling Mistakes"
      ],
      "metadata": {
        "id": "2NgCS1PQZaIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW5kZQP-W3CE"
      },
      "outputs": [],
      "source": [
        "def Check_Spelling(Sentence):\n",
        "  \"\"\"\n",
        "    Checks spelling of each word\n",
        "\n",
        "    Args:\n",
        "      word: Words (Tokens) of each essay \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  Sentence = word_tokenize(Sentence)\n",
        "  for word in Sentence:\n",
        "    word = Word(word)\n",
        "  \n",
        "    result = word.spellcheck()\n",
        "\n",
        "    # result [0][0] contains the bool value if the spelling is correct or not\n",
        "    # result [0][1] contains the confidence for the suggest correct spelling\n",
        "\n",
        "    if word != result[0][0]:\n",
        "      if(result[0][1] > 0.95 and not(wordnet.synsets(word)) and not(\"/\" in word)):\n",
        "        print(word , result[0][0])\n",
        "        count = count + 1\n",
        "\n",
        "  return count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Preprocessed_Essay\"] = Data_Essay_01[\"Essay\"].apply(Remove_NER)\n",
        "Data_Essay_01[\"Preprocessed_Essay\"] = Data_Essay_01[\"Preprocessed_Essay\"].apply(Remove_Punctuations)"
      ],
      "metadata": {
        "id": "Lhbvn0kuFGnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Spelling_Mistakes_Count\"]  = Data_Essay_01[\"Preprocessed_Essay\"].map(Check_Spelling)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "u5UO3OIYE-8p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31a8a1e4-064c-4438-eae4-93af3a6a07ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "troble trouble\n",
            "buisness business\n",
            "myspace space\n",
            "forbidde forbidden\n",
            "troble trouble\n",
            "coordibates coordinate\n",
            "myspace space\n",
            "coordibates coordinate\n",
            "appling applying\n",
            "benifit benefit\n",
            "studdies studies\n",
            "reasearch research\n",
            "advertisments advertisements\n",
            "imposible impossible\n",
            "reasearch research\n",
            "reasearch research\n",
            "posible possible\n",
            "amagine imagine\n",
            "buissness business\n",
            "catalouge catalogue\n",
            "castomer customer\n",
            "coustomers customers\n",
            "resturant restaurant\n",
            "convinences convinces\n",
            "reaserch research\n",
            "Computors Computers\n",
            "computors computers\n",
            "computors computers\n",
            "convience convince\n",
            "confrence conference\n",
            "parttners partners\n",
            "conviente convient\n",
            "subssiquently subsequently\n",
            "conviente convient\n",
            "camputer computer\n",
            "posibility possibility\n",
            "everthing everything\n",
            "wouldbe would\n",
            "phisically physically\n",
            "ardthritis arthritis\n",
            "myspace space\n",
            "embarrasing embarrassing\n",
            "feild field\n",
            "everyones everyone\n",
            "Ther Her\n",
            "atleast least\n",
            "obease obese\n",
            "problum problem\n",
            "socioty society\n",
            "obease obese\n",
            "cumputers computers\n",
            "amarica america\n",
            "exersise exercise\n",
            "computeres computers\n",
            "mealting melting\n",
            "electricty electricity\n",
            "thjen then\n",
            "electrisity electricity\n",
            "happining happening\n",
            "shorline shrine\n",
            "technolagy technology\n",
            "peroson person\n",
            "themselfs themselves\n",
            "conclution conclusion\n",
            "computeres computers\n",
            "amarica america\n",
            "obease obese\n",
            "reson reason\n",
            "califan halifax\n",
            "exirsis exists\n",
            "thrid third\n",
            "reson reason\n",
            "peolpe people\n",
            "exmple example\n",
            "adout about\n",
            "consiter consider\n",
            "thrie three\n",
            "resons reasons\n",
            "resons reasons\n",
            "postition position\n",
            "bigin begin\n",
            "insted instead\n",
            "insted instead\n",
            "incomputer computer\n",
            "becasue because\n",
            "resons reasons\n",
            "futur future\n",
            "resons reasons\n",
            "bigin begin\n",
            "insted instead\n",
            "abd and\n",
            "insted instead\n",
            "cmputer computer\n",
            "papaer paper\n",
            "resons reasons\n",
            "papaer paper\n",
            "Thers Hers\n",
            "interection interaction\n",
            "nessessity necessity\n",
            "soceity society\n",
            "usod used\n",
            "Additionaly Additional\n",
            "reasearch research\n",
            "peole people\n",
            "peole people\n",
            "scaresly scarcely\n",
            "Oviously Obviously\n",
            "foreward forward\n",
            "bility ability\n",
            "technologicall technological\n",
            "belive believe\n",
            "belive believe\n",
            "belive believe\n",
            "exersise exercise\n",
            "belive believe\n",
            "addictted addicted\n",
            "pleanty plenty\n",
            "belive believe\n",
            "centainly certainly\n",
            "joggin join\n",
            "workin working\n",
            "thats that\n",
            "froi from\n",
            "invanyor inventor\n",
            "leki levi\n",
            "mashine machine\n",
            "reasearch research\n",
            "desiner designer\n",
            "communtiy community\n",
            "dieseases diseases\n",
            "neccessitys necessity\n",
            "myspace space\n",
            "youtube couture\n",
            "recestion reception\n",
            "exersice exercise\n",
            "exersice exercise\n",
            "exersice exercise\n",
            "foregin foreign\n",
            "tye the\n",
            "thats that\n",
            "yu you\n",
            "wih with\n",
            "imprdove improve\n",
            "beileve believe\n",
            "helpfull helpful\n",
            "benifit benefit\n",
            "diffrent different\n",
            "refferances references\n",
            "adicted addicted\n",
            "beleive believe\n",
            "infermation information\n",
            "infermation information\n",
            "wepons weapons\n",
            "differnt different\n",
            "goind going\n",
            "whats what\n",
            "goind going\n",
            "computres computers\n",
            "computres computers\n",
            "computres computers\n",
            "deffinately definitely\n",
            "webkinz webbing\n",
            "usefull useful\n",
            "invatations invitations\n",
            "thats that\n",
            "colloge college\n",
            "negitive negative\n",
            "myspace space\n",
            "unseatled unsettled\n",
            "friesnd friend\n",
            "understandd understand\n",
            "becuase because\n",
            "benifit benefit\n",
            "intach intact\n",
            "orginized organized\n",
            "diliberatly deliberately\n",
            "sesonal seasonal\n",
            "thats that\n",
            "intouch touch\n",
            "bestfriend befriend\n",
            "vidio video\n",
            "intouch touch\n",
            "needey needed\n",
            "intouch touch\n",
            "computor computer\n",
            "opition opinion\n",
            "havent haven\n",
            "advsnces advances\n",
            "excercising exercising\n",
            "becacuse because\n",
            "nieghborhood neighborhood\n",
            "whats what\n",
            "chuckie chuckle\n",
            "naylor taylor\n",
            "wacth watch\n",
            "belive believe\n",
            "negitive negative\n",
            "taht that\n",
            "likly likely\n",
            "likly likely\n",
            "probibly probably\n",
            "probibly probably\n",
            "excersize excessive\n",
            "likly likely\n",
            "likly likely\n",
            "iTunes tunes\n",
            "defenetly decently\n",
            "sergents sergeants\n",
            "enternet internet\n",
            "myspace space\n",
            "computeres computers\n",
            "beneifts benefits\n",
            "sosiety society\n",
            "reson reason\n",
            "undersand understand\n",
            "frends friends\n",
            "reson reason\n",
            "youtube couture\n",
            "obise obese\n",
            "thats that\n",
            "buissness business\n",
            "Celebraties Celebrates\n",
            "probaly probably\n",
            "sconclusion conclusion\n",
            "computor computer\n",
            "havent haven\n",
            "enternet internet\n",
            "havent haven\n",
            "fingure figure\n",
            "beleive believe\n",
            "beleive believe\n",
            "cordonation coronation\n",
            "probobility probability\n",
            "probobility probability\n",
            "vacational vocational\n",
            "machinary machinery\n",
            "buisness business\n",
            "cordonation coronation\n",
            "buisness business\n",
            "wellfare welfare\n",
            "xbox box\n",
            "advanement advancement\n",
            "benifit benefit\n",
            "completly completely\n",
            "definatly defiantly\n",
            "benifits benefits\n",
            "havn have\n",
            "acheive achieve\n",
            "completly completely\n",
            "thats that\n",
            "chatins chains\n",
            "labtop labor\n",
            "chatins chains\n",
            "Paretns Parents\n",
            "truely truly\n",
            "possesion possession\n",
            "aperson person\n",
            "concentrat concentrate\n",
            "poppular popular\n",
            "technolgy technology\n",
            "exercis exercise\n",
            "mounthly monthly\n",
            "beleive believe\n",
            "thats that\n",
            "differnt different\n",
            "differnt different\n",
            "differnt different\n",
            "scheduale schedule\n",
            "anytime daytime\n",
            "releave release\n",
            "friendsand friedland\n",
            "thats that\n",
            "peole people\n",
            "theior their\n",
            "whats what\n",
            "myspace space\n",
            "thats that\n",
            "excercising exercising\n",
            "coubtries countries\n",
            "excercise exercise\n",
            "converstions conversations\n",
            "ooVoo oooh\n",
            "prounociation pronunciation\n",
            "langages languages\n",
            "havent haven\n",
            "memebers members\n",
            "myspace space\n",
            "technologie technology\n",
            "technologie technology\n",
            "technologie technology\n",
            "technologie technology\n",
            "laplops gallops\n",
            "understandds understands\n",
            "groth growth\n",
            "helpul helpful\n",
            "saids said\n",
            "vidieo video\n",
            "insted instead\n",
            "Comuters Computers\n",
            "buissness business\n",
            "uselful useful\n",
            "buissness business\n",
            "comuters computers\n",
            "buissness business\n",
            "becasue because\n",
            "proceeses processes\n",
            "phycologist psychologist\n",
            "speciaizes specialized\n",
            "obesse obese\n",
            "rayfield hayfield\n",
            "arguements arguments\n",
            "thru thou\n",
            "acutually actually\n",
            "Turst Burst\n",
            "benifits benefits\n",
            "efficent efficient\n",
            "arrands errands\n",
            "millons millions\n",
            "Futhermore Furthermore\n",
            "intouch touch\n",
            "plummit summit\n",
            "benifited benefited\n",
            "excersice excessive\n",
            "definatelly definitely\n",
            "definately definitely\n",
            "healthey healthy\n",
            "thier their\n",
            "Unfortunatly Unfortunately\n",
            "aspact aspect\n",
            "ethier ether\n",
            "extremly extremely\n",
            "thier their\n",
            "xbox box\n",
            "playstation plantation\n",
            "xbox box\n",
            "psp pp\n",
            "gameboy pageboy\n",
            "xbox box\n",
            "olny only\n",
            "thier their\n",
            "thier their\n",
            "togther together\n",
            "thier their\n",
            "exprience experience\n",
            "oppinion opinion\n",
            "gramar grammar\n",
            "benifical beneficial\n",
            "libary library\n",
            "benifit benefit\n",
            "comunication communication\n",
            "comunication communication\n",
            "benifit benefit\n",
            "benifit benefit\n",
            "cridit credit\n",
            "insted instead\n",
            "outdores outdone\n",
            "passtime pastime\n",
            "oppertunity opportunity\n",
            "understod understood\n",
            "concideration consideration\n",
            "coltures cultures\n",
            "favert avert\n",
            "benifit benefit\n",
            "benifit benefit\n",
            "dieabeties diabetes\n",
            "dissapointed disappointed\n",
            "everthing everything\n",
            "dissapointed disappointed\n",
            "safty safety\n",
            "safty safety\n",
            "thats that\n",
            "safty safety\n",
            "stillis stills\n",
            "benifit benefit\n",
            "comunication communication\n",
            "benifit benefit\n",
            "proffessions professions\n",
            "acessable accessible\n",
            "benifets benefits\n",
            "benifits benefits\n",
            "benifits benefits\n",
            "inbox into\n",
            "priveleges privileges\n",
            "vistoriously victoriously\n",
            "eith with\n",
            "deffinition definition\n",
            "helpfull helpful\n",
            "helpfull helpful\n",
            "helpfull helpful\n",
            "helpfull helpful\n",
            "helpfull helpful\n",
            "vaction action\n",
            "vaction action\n",
            "benifit benefit\n",
            "benifits benefits\n",
            "succesful successful\n",
            "insted instead\n",
            "easilly easily\n",
            "lthat that\n",
            "colege college\n",
            "aol all\n",
            "spo so\n",
            "benifets benefits\n",
            "esencial essential\n",
            "benificial beneficial\n",
            "goverment government\n",
            "immensly immensely\n",
            "vincinity vicinity\n",
            "posotive positive\n",
            "othe the\n",
            "incomg income\n",
            "definetly definitely\n",
            "whats what\n",
            "comunication communication\n",
            "buisnoss business\n",
            "hardely hardly\n",
            "ooVoo oooh\n",
            "comunication communication\n",
            "reasearch research\n",
            "youtube couture\n",
            "excersize excessive\n",
            "computors computers\n",
            "computors computers\n",
            "computors computers\n",
            "computors computers\n",
            "anyting anything\n",
            "computors computers\n",
            "computor computer\n",
            "extremly extremely\n",
            "computors computers\n",
            "asword sword\n",
            "computors computers\n",
            "Computors Computers\n",
            "computors computers\n",
            "goting going\n",
            "goting going\n",
            "childern children\n",
            "childerns children\n",
            "childern children\n",
            "eyesit eyes\n",
            "exercis exercise\n",
            "thats that\n",
            "atleast least\n",
            "youtube couture\n",
            "outsite outside\n",
            "youtube couture\n",
            "becuse because\n",
            "nght night\n",
            "wheter whether\n",
            "effets effects\n",
            "mabey mabel\n",
            "peopl people\n",
            "efficiant efficient\n",
            "efficiant efficient\n",
            "efficiant efficient\n",
            "ina in\n",
            "recquire require\n",
            "buisness business\n",
            "useage usage\n",
            "aplications applications\n",
            "commertials commercial\n",
            "avertisements advertisements\n",
            "conclution conclusion\n",
            "expirience experience\n",
            "expirience experience\n",
            "excercising exercising\n",
            "immediatly immediately\n",
            "enviroment environment\n",
            "enviroment environment\n",
            "youtube couture\n",
            "inappronrate inappropriate\n",
            "oppisite opposite\n",
            "verbily verily\n",
            "amarica america\n",
            "batterie batteries\n",
            "Simulaters Simulates\n",
            "controll control\n",
            "possitive positive\n",
            "comunicate communicate\n",
            "baout about\n",
            "happended happened\n",
            "sudents students\n",
            "everyones everyone\n",
            "comunicating communicating\n",
            "gratest greatest\n",
            "diffuclty difficulty\n",
            "libery liberty\n",
            "libery liberty\n",
            "libery liberty\n",
            "lvl ll\n",
            "lvl ll\n",
            "benificial beneficial\n",
            "thngs things\n",
            "ecetra extra\n",
            "posistion position\n",
            "migrane migrate\n",
            "compuer computer\n",
            "satisying satisfying\n",
            "learnen learned\n",
            "myspace space\n",
            "youtube couture\n",
            "mspace space\n",
            "myspace space\n",
            "youtube couture\n",
            "famly family\n",
            "chidren children\n",
            "coumputer computer\n",
            "famly family\n",
            "Youtube Couture\n",
            "asom atom\n",
            "freinds friends\n",
            "famly family\n",
            "chus thus\n",
            "adicted addicted\n",
            "benefiticial beneficial\n",
            "saearch search\n",
            "phnes panes\n",
            "pne one\n",
            "anytime daytime\n",
            "usefull useful\n",
            "helpfull helpful\n",
            "doesnt doesn\n",
            "helpfull helpful\n",
            "exersising exercising\n",
            "comptuers computers\n",
            "addictied addicted\n",
            "predetors creditors\n",
            "kidnapp kidnap\n",
            "recognation recognition\n",
            "childrens children\n",
            "thats that\n",
            "fantacy fantasy\n",
            "assumtion assumption\n",
            "fantacy fantasy\n",
            "justto justo\n",
            "comminicate communicate\n",
            "incontact contact\n",
            "proposterous preposterous\n",
            "disaters disasters\n",
            "whatevers whatever\n",
            "intervied interview\n",
            "compuer computer\n",
            "controversal controversial\n",
            "enourmous enormous\n",
            "benifits benefits\n",
            "benefitial beneficial\n",
            "scoliousis scoliosis\n",
            "excercising exercising\n",
            "keepin keeping\n",
            "gyou you\n",
            "eople people\n",
            "exersice exercise\n",
            "needds needs\n",
            "intouch touch\n",
            "woulden wouldn\n",
            "myspace space\n",
            "aol all\n",
            "resturant restaurant\n",
            "conculsion conclusion\n",
            "thats that\n",
            "ohter other\n",
            "mohter mother\n",
            "conclusing concluding\n",
            "multuplication multiplication\n",
            "desaster disaster\n",
            "whats what\n",
            "discusion discussion\n",
            "reconize recognize\n",
            "certin certain\n",
            "voide voice\n",
            "Cybe Be\n",
            "priorites priority\n",
            "thats that\n",
            "thats that\n",
            "thats that\n",
            "purphases purchases\n",
            "convinent continent\n",
            "soliders soldiers\n",
            "soliders soldiers\n",
            "usefull useful\n",
            "wasnt want\n",
            "prodject project\n",
            "usinf using\n",
            "expessaly expressly\n",
            "exersizing exercising\n",
            "exersizes exercises\n",
            "possitive positive\n",
            "curcial crucial\n",
            "dependancy dependency\n",
            "ocmputer computer\n",
            "comptuer computer\n",
            "interacte interact\n",
            "exersise exercise\n",
            "oerson person\n",
            "youtube couture\n",
            "adminitrating administrating\n",
            "loacl local\n",
            "newapaper newspaper\n",
            "newpaper newspaper\n",
            "govermnet government\n",
            "hignway highway\n",
            "Prctically Practically\n",
            "aol all\n",
            "administrater administrator\n",
            "reseach research\n",
            "assitance assistance\n",
            "ahving having\n",
            "naetly neatly\n",
            "communiacte communicate\n",
            "usieng using\n",
            "becuse because\n",
            "ehey they\n",
            "becuse because\n",
            "corect correct\n",
            "becuse because\n",
            "ingriedient ingredient\n",
            "belive believe\n",
            "benifit benefit\n",
            "everywere everywhere\n",
            "expeirences experiences\n",
            "freinds friends\n",
            "benifit benefit\n",
            "polution solution\n",
            "benifit benefit\n",
            "benifit benefit\n",
            "myspace space\n",
            "myspace space\n",
            "somy some\n",
            "canda canada\n",
            "injoy enjoy\n",
            "Inconclusion Conclusion\n",
            "effeck effect\n",
            "magnificant magnificent\n",
            "exsist exist\n",
            "resomes resumes\n",
            "understeading understanding\n",
            "animales animals\n",
            "anythin anything\n",
            "somethine something\n",
            "vocab vocal\n",
            "vocab vocal\n",
            "privite private\n",
            "infenite infinite\n",
            "nolege college\n",
            "knoledge knowledge\n",
            "socialy social\n",
            "congradulate congratulate\n",
            "cocnclusion conclusion\n",
            "exercing exerting\n",
            "likly likely\n",
            "hospitial hospital\n",
            "surgen surgeon\n",
            "grammer grammar\n",
            "doesnt doesn\n",
            "internent internet\n",
            "beleive believe\n",
            "benefil benefit\n",
            "buisness business\n",
            "buisness business\n",
            "buisness business\n",
            "buisness business\n",
            "Buisness Business\n",
            "togethers together\n",
            "imediately immediately\n",
            "excercise exercise\n",
            "expirience experience\n",
            "indentity identity\n",
            "thats that\n",
            "farmpring farming\n",
            "thats that\n",
            "texting testing\n",
            "withb with\n",
            "woithout without\n",
            "whcih which\n",
            "graetly greatly\n",
            "controll control\n",
            "outlowed outlawed\n",
            "exersizing exercising\n",
            "adicted addicted\n",
            "myspace space\n",
            "incroas increase\n",
            "affenders offenders\n",
            "tramendus tremendous\n",
            "Teh Eh\n",
            "comkputer computer\n",
            "suposed supposed\n",
            "myspace space\n",
            "everday everyday\n",
            "presuade persuade\n",
            "completley completely\n",
            "snowday snowy\n",
            "cancled canceled\n",
            "woulden wouldn\n",
            "posative positive\n",
            "avalible available\n",
            "messanger messenger\n",
            "youre your\n",
            "ebay bay\n",
            "alls all\n",
            "comuter computer\n",
            "camputers computers\n",
            "excersize excessive\n",
            "comuter computer\n",
            "cordanation coronation\n",
            "compters computers\n",
            "socity society\n",
            "asignments assignment\n",
            "diffrent different\n",
            "mnake make\n",
            "parrents parents\n",
            "parrents parents\n",
            "ofering offering\n",
            "oppertunities opportunities\n",
            "oppertunities opportunities\n",
            "contraversal controversial\n",
            "adolesents adolescents\n",
            "adavantages advantages\n",
            "exercizing exercising\n",
            "thats that\n",
            "thier their\n",
            "someitmes sometimes\n",
            "paidless painless\n",
            "woulden wouldn\n",
            "deppression depression\n",
            "benifit benefit\n",
            "everyting everything\n",
            "somone someone\n",
            "graphis graphics\n",
            "aritist artist\n",
            "acitivitys activity\n",
            "benifits benefits\n",
            "securty security\n",
            "coumputers computers\n",
            "fourmula formula\n",
            "graphis graphics\n",
            "coumputers computers\n",
            "graphis graphics\n",
            "coumputers computers\n",
            "coumputor computer\n",
            "graphis graphics\n",
            "coumputer computer\n",
            "acivitys activity\n",
            "coumputors computers\n",
            "coumputors computers\n",
            "youtube couture\n",
            "youtube couture\n",
            "coumputor computer\n",
            "coumputors computers\n",
            "bennafit benefit\n",
            "exstent extent\n",
            "collassal colossal\n",
            "definetly definitely\n",
            "possitive positive\n",
            "myspace space\n",
            "efferct effect\n",
            "childre children\n",
            "instintly instantly\n",
            "advanrages advantages\n",
            "imformation information\n",
            "reson reason\n",
            "interfear interfere\n",
            "familliar familiar\n",
            "unpleasent unpleasant\n",
            "imformation information\n",
            "ciber cider\n",
            "imformation information\n",
            "doesnt doesn\n",
            "peole people\n",
            "veiwible visible\n",
            "piont point\n",
            "magnificient magnificent\n",
            "thats that\n",
            "understant understand\n",
            "heldful helpful\n",
            "exercis exercise\n",
            "exercis exercise\n",
            "whats what\n",
            "depresion depression\n",
            "aol all\n",
            "allways always\n",
            "allways always\n",
            "everday everyday\n",
            "ontire entire\n",
            "exerising exercising\n",
            "beacause because\n",
            "myspace space\n",
            "instad instead\n",
            "youtube couture\n",
            "laughin laughing\n",
            "enoughb enough\n",
            "lcomputer computer\n",
            "internent internet\n",
            "youtube couture\n",
            "myspace space\n",
            "reasearch research\n",
            "enywhere anywhere\n",
            "inordre ordre\n",
            "Encouragment Encouragement\n",
            "goverment government\n",
            "relationshops relationships\n",
            "thats that\n",
            "whats what\n",
            "posotive positive\n",
            "familly family\n",
            "definatly defiantly\n",
            "conveniant convenient\n",
            "helpul helpful\n",
            "faraways faraway\n",
            "onlinee online\n",
            "helpul helpful\n",
            "ccan can\n",
            "impaced impacted\n",
            "millenium millennium\n",
            "intellegence intelligence\n",
            "appreciacion appreciation\n",
            "onlr only\n",
            "enbough enough\n",
            "dissapear disappear\n",
            "oppinion opinion\n",
            "allways always\n",
            "lillte little\n",
            "myspace space\n",
            "sutch such\n",
            "progrem program\n",
            "thease these\n",
            "necesary necessary\n",
            "greaduate graduate\n",
            "epidemin epidemic\n",
            "anegative negative\n",
            "remeber remember\n",
            "infromation information\n",
            "liesure leisure\n",
            "fogetting forgetting\n",
            "aweful awful\n",
            "experinced experienced\n",
            "adictied addicted\n",
            "faton baton\n",
            "unhealth unhealthy\n",
            "unfortunet unfortunate\n",
            "ourself yourself\n",
            "dilema dilemma\n",
            "othe the\n",
            "proffestional professional\n",
            "coutries countries\n",
            "whats what\n",
            "whats what\n",
            "comunicating communicating\n",
            "invation invasion\n",
            "straightin straighten\n",
            "seperate separate\n",
            "reasearch research\n",
            "excercise exercise\n",
            "beuty beauty\n",
            "truely truly\n",
            "accoding according\n",
            "hunington huntington\n",
            "youself yourself\n",
            "ourside outside\n",
            "Yah Ah\n",
            "youre your\n",
            "disapear disappear\n",
            "sendin sending\n",
            "happends happens\n",
            "thats that\n",
            "promblems problems\n",
            "otherside otherwise\n",
            "thats that\n",
            "peope people\n",
            "troppical tropical\n",
            "thier their\n",
            "resorce resource\n",
            "compoler composer\n",
            "qulities qualities\n",
            "buisness business\n",
            "exersize exercise\n",
            "insed rinsed\n",
            "actultly actually\n",
            "peopel people\n",
            "everday everyday\n",
            "vertual virtual\n",
            "causesan causes\n",
            "diabeaties diabetes\n",
            "canser cancer\n",
            "enouhg enough\n",
            "didi did\n",
            "disappering disappearing\n",
            "envieronmental environmental\n",
            "completly completely\n",
            "acessive excessive\n",
            "comuters computers\n",
            "everyones everyone\n",
            "anytime daytime\n",
            "fmily family\n",
            "texting testing\n",
            "texting testing\n",
            "whtm whom\n",
            "hpoe hope\n",
            "socia social\n",
            "thier their\n",
            "thier their\n",
            "secend second\n",
            "thats that\n",
            "thats that\n",
            "wouldent wouldn\n",
            "Ccomputers Computers\n",
            "benifit benefit\n",
            "pensil pencil\n",
            "emproved improved\n",
            "magaziene magazine\n",
            "Importan Important\n",
            "sumtimes sometimes\n",
            "wouldent wouldn\n",
            "websights weights\n",
            "uptodate update\n",
            "presedent president\n",
            "vidio video\n",
            "everyware everywhere\n",
            "Emale Male\n",
            "coworkers workers\n",
            "somthing something\n",
            "amaricans americans\n",
            "exersize exercise\n",
            "exersise exercise\n",
            "arthrities arthritis\n",
            "amke make\n",
            "bacj back\n",
            "cordanation coronation\n",
            "cordanation coronation\n",
            "whats what\n",
            "Coumputers Computers\n",
            "soemone someone\n",
            "cordniation coronation\n",
            "opertunities opportunities\n",
            "and… and\n",
            "excersize excessive\n",
            "fasinate fascinate\n",
            "excersising exercising\n",
            "woudl would\n",
            "sugested suggested\n",
            "anouther another\n",
            "detatched detached\n",
            "convenvent convenient\n",
            "skped sped\n",
            "acctually actually\n",
            "geoagrahy geography\n",
            "voilence violence\n",
            "voilence violence\n",
            "voilence violence\n",
            "concenred concerned\n",
            "voilence violence\n",
            "unnatral unnatural\n",
            "myspace space\n",
            "thats that\n",
            "butey bute\n",
            "soemone someone\n",
            "thats that\n",
            "wern were\n",
            "heppening happening\n",
            "exsistence existence\n",
            "spreding spreading\n",
            "ichat chat\n",
            "istead instead\n",
            "atleast least\n",
            "nuscense suspense\n",
            "wern were\n",
            "texting testing\n",
            "resuraunt restraint\n",
            "Whilesome Wholesome\n",
            "finiding finding\n",
            "consequencial consequential\n",
            "injoy enjoy\n",
            "vidio video\n",
            "pepole people\n",
            "semplr semple\n",
            "depnding depending\n",
            "deforents deferens\n",
            "informashon information\n",
            "everiday everyday\n",
            "mayself myself\n",
            "pepole people\n",
            "abyous about\n",
            "importen imported\n",
            "effact effect\n",
            "haus has\n",
            "weth with\n",
            "complateley completely\n",
            "newsepaper newspaper\n",
            "shouldnt shouldn\n",
            "dailey daily\n",
            "litsen listen\n",
            "cordinations combinations\n",
            "dailey daily\n",
            "havea have\n",
            "shouldnt shouldn\n",
            "elsa else\n",
            "contrey contre\n",
            "Seart Heart\n",
            "exercis exercise\n",
            "youtube couture\n",
            "lison lion\n",
            "musick music\n",
            "thats that\n",
            "febble feeble\n",
            "Theese Cheese\n",
            "visiion vision\n",
            "excercize exercise\n",
            "exercizing exercising\n",
            "bacause because\n",
            "spenfing spending\n",
            "exercis exercise\n",
            "comuter computer\n",
            "waould would\n",
            "assigment assignment\n",
            "wuth with\n",
            "whats what\n",
            "wwe we\n",
            "thnigs things\n",
            "bissy missy\n",
            "advertisment advertisement\n",
            "realiable reliable\n",
            "shoud should\n",
            "attatched attached\n",
            "interasting interesting\n",
            "belive believe\n",
            "kthat that\n",
            "imformation information\n",
            "truning turning\n",
            "exerising exercising\n",
            "freish fresh\n",
            "beofre before\n",
            "youre your\n",
            "computres computers\n",
            "myspace space\n",
            "haapy happy\n",
            "reasch reach\n",
            "nooding nodding\n",
            "exericis exercise\n",
            "vaction action\n",
            "vaction action\n",
            "benifit benefit\n",
            "devastaded devastated\n",
            "ohter other\n",
            "futur future\n",
            "solitare solitary\n",
            "peole people\n",
            "benifit benefit\n",
            "Somethings Something\n",
            "purchesed purchased\n",
            "pently gently\n",
            "benifit benefit\n",
            "obise obese\n",
            "Commmunities Communities\n",
            "commputer computer\n",
            "unneccessary unnecessary\n",
            "talknig talking\n",
            "devorce divorce\n",
            "excercising exercising\n",
            "buit but\n",
            "soceiety society\n",
            "troble trouble\n",
            "thease these\n",
            "culters cutters\n",
            "obeast beast\n",
            "unhealth unhealthy\n",
            "whatch watch\n",
            "atleast least\n",
            "whatch watch\n",
            "whatch watch\n",
            "familiy family\n",
            "ethier ether\n",
            "aol all\n",
            "myspace space\n",
            "thats that\n",
            "freind friend\n",
            "freind friend\n",
            "bestfriend befriend\n",
            "thats that\n",
            "thats that\n",
            "excersising exercising\n",
            "excersising exercising\n",
            "atleast least\n",
            "buttox button\n",
            "youtube couture\n",
            "excercise exercise\n",
            "preditors creditors\n",
            "useage usage\n",
            "isee see\n",
            "opinon opinion\n",
            "myspace space\n",
            "internent internet\n",
            "internent internet\n",
            "myspace space\n",
            "carryied carried\n",
            "shoud should\n",
            "internent internet\n",
            "earlyier earlier\n",
            "earlyier earlier\n",
            "internent internet\n",
            "elimate climate\n",
            "internent internet\n",
            "internent internet\n",
            "earlyier earlier\n",
            "quility quality\n",
            "interraction interaction\n",
            "additon addition\n",
            "hatefull hateful\n",
            "excersice excessive\n",
            "acumulate accumulate\n",
            "excersice excessive\n",
            "asures assures\n",
            "themsleves themselves\n",
            "excersice excessive\n",
            "imense immense\n",
            "convinent continent\n",
            "asigns signs\n",
            "convinent continent\n",
            "convinent continent\n",
            "convinent continent\n",
            "enteratinment entertainment\n",
            "succesful successful\n",
            "conlude conclude\n",
            "evoloution evolution\n",
            "ordinarilly ordinarily\n",
            "Insteed Instead\n",
            "myspace space\n",
            "youtube couture\n",
            "myspace space\n",
            "interseted interested\n",
            "strints strings\n",
            "newpapers newspapers\n",
            "exsample example\n",
            "thats that\n",
            "exsample example\n",
            "sosme some\n",
            "exsample example\n",
            "lette letter\n",
            "eveything everything\n",
            "positve positive\n",
            "atleast least\n",
            "whats what\n",
            "freinds friends\n",
            "comercials commercial\n",
            "wernt went\n",
            "theese these\n",
            "whats what\n",
            "scientest scientist\n",
            "thats that\n",
            "memorr memory\n",
            "relfexes reflexes\n",
            "complexing completing\n",
            "raflexes reflexes\n",
            "encouregement encouragement\n",
            "thw the\n",
            "obiese obese\n",
            "cna can\n",
            "bettre better\n",
            "heloing helping\n",
            "tecnology technology\n",
            "simpiler simpler\n",
            "simpling dimpling\n",
            "knowlage knowledge\n",
            "knowlage knowledge\n",
            "allready already\n",
            "knowlage knowledge\n",
            "posotive positive\n",
            "usefull useful\n",
            "vocab vocal\n",
            "accesible accessible\n",
            "exersise exercise\n",
            "necissary necessary\n",
            "accross across\n",
            "accross across\n",
            "accross across\n",
            "buisness business\n",
            "thats that\n",
            "lof of\n",
            "comptuer computer\n",
            "helpul helpful\n",
            "myspace space\n",
            "naother another\n",
            "helpul helpful\n",
            "samething something\n",
            "clastering clattering\n",
            "smaler smaller\n",
            "definetly definitely\n",
            "youtube couture\n",
            "waterski waters\n",
            "lthink think\n",
            "benifit benefit\n",
            "horriable horrible\n",
            "moneged moneyed\n",
            "termendous tremendous\n",
            "termendous tremendous\n",
            "wpould would\n",
            "thats that\n",
            "gracous gracious\n",
            "exausted exhausted\n",
            "exercizing exercising\n",
            "exercize exercise\n",
            "thier their\n",
            "insteads instead\n",
            "communicte communicate\n",
            "comuputers computers\n",
            "soemone someone\n",
            "privalige privilege\n",
            "othe the\n",
            "expiriment experiment\n",
            "therew there\n",
            "positivly positively\n",
            "positivley positively\n",
            "symtom symptom\n",
            "asignments assignment\n",
            "inhance enhance\n",
            "excrise excise\n",
            "depating departing\n",
            "thrie three\n",
            "childern children\n",
            "childern children\n",
            "thier their\n",
            "sosial social\n",
            "recuar recur\n",
            "childern children\n",
            "ablt able\n",
            "apointment appointment\n",
            "societey society\n",
            "sociatey society\n",
            "beautful beautiful\n",
            "itto into\n",
            "publushers publishers\n",
            "sociatey society\n",
            "socitey society\n",
            "communicat communicate\n",
            "sociatey society\n",
            "excercising exercising\n",
            "lettng letting\n",
            "micht might\n",
            "nonsens nonsense\n",
            "excercise exercise\n",
            "ahppen happen\n",
            "excercise exercise\n",
            "usefull useful\n",
            "certian certain\n",
            "usefull useful\n",
            "lopez lope\n",
            "helpfull helpful\n",
            "myspace space\n",
            "addrees address\n",
            "usefull useful\n",
            "boest best\n",
            "internat internal\n",
            "benifits benefits\n",
            "ebay bay\n",
            "accorucy accuracy\n",
            "thats that\n",
            "compouters computers\n",
            "excersising exercising\n",
            "scenary scenery\n",
            "copmuters computers\n",
            "beatiful beautiful\n",
            "unactive inactive\n",
            "myspace space\n",
            "havent haven\n",
            "fashoned fashioned\n",
            "cnn can\n",
            "myspace space\n",
            "acroos across\n",
            "conslusion conclusion\n",
            "aquiring acquiring\n",
            "terribele terrible\n",
            "belive believe\n",
            "exersizing exercising\n",
            "languege language\n",
            "youre your\n",
            "excersize excessive\n",
            "thats that\n",
            "especialy especially\n",
            "excersize excessive\n",
            "belive believe\n",
            "teamate teamster\n",
            "wouldnt wouldn\n",
            "furter further\n",
            "splittling splitting\n",
            "comuter computer\n",
            "emeil email\n",
            "websit west\n",
            "websit west\n",
            "enythiny anything\n",
            "inportent important\n",
            "websit west\n",
            "benifit benefit\n",
            "rember member\n",
            "werent weren\n",
            "diffrent different\n",
            "mainiy mainly\n",
            "benifit benefit\n",
            "ahmazing amazing\n",
            "becaus because\n",
            "relize realize\n",
            "thats that\n",
            "somtim sometime\n",
            "ofs of\n",
            "benifit benefit\n",
            "socity society\n",
            "definatly defiantly\n",
            "benifit benefit\n",
            "socity society\n",
            "comunications communications\n",
            "redily readily\n",
            "avalible available\n",
            "nessesary necessary\n",
            "commputer computer\n",
            "relible reliable\n",
            "relible reliable\n",
            "benifit benefit\n",
            "socity society\n",
            "evrywhere everywhere\n",
            "evrything everything\n",
            "paragrahs paragraphs\n",
            "myspace space\n",
            "adicted addicted\n",
            "thier their\n",
            "abandonned abandoned\n",
            "alchohol alcohol\n",
            "youre your\n",
            "youre your\n",
            "webking webbing\n",
            "heathy healthy\n",
            "restaraunts restraints\n",
            "playstation plantation\n",
            "libary library\n",
            "bounddries boundaries\n",
            "nearily nearly\n",
            "inlike unlike\n",
            "practic practice\n",
            "importan important\n",
            "belive believe\n",
            "knowlege knowledge\n",
            "knowlege knowledge\n",
            "knowlege knowledge\n",
            "availible available\n",
            "knowlege knowledge\n",
            "exencise exercise\n",
            "knowlege knowledge\n",
            "desasters disasters\n",
            "knowlege knowledge\n",
            "knowlege knowledge\n",
            "knowlege knowledge\n",
            "aldults adults\n",
            "especialy especially\n",
            "penale penal\n",
            "disaprove disapprove\n",
            "somthing something\n",
            "durectly directly\n",
            "studees studies\n",
            "withoout without\n",
            "libary library\n",
            "thats that\n",
            "logon login\n",
            "libary library\n",
            "fundimg funding\n",
            "imparing impairing\n",
            "reconect recollect\n",
            "unwadged unwanted\n",
            "eartquake earthquake\n",
            "accross across\n",
            "deffinition definition\n",
            "passtime pastime\n",
            "acually actually\n",
            "teachees teachers\n",
            "passtimes pastime\n",
            "thje the\n",
            "Futhermore Furthermore\n",
            "cerrect correct\n",
            "Tecenology Technology\n",
            "benifits benefits\n",
            "iportant important\n",
            "swiching swishing\n",
            "econimy economy\n",
            "athe the\n",
            "freinds friends\n",
            "myspace space\n",
            "goto got\n",
            "educatinola educational\n",
            "possitive positive\n",
            "possitive positive\n",
            "Thiss Hiss\n",
            "dagerous dangerous\n",
            "excersising exercising\n",
            "Excercising Exercising\n",
            "somthing something\n",
            "cluter cluster\n",
            "bestfriend befriend\n",
            "Compaines Companies\n",
            "curent current\n",
            "thats that\n",
            "myspace space\n",
            "activites activities\n",
            "goto got\n",
            "sititng sitting\n",
            "excercising exercising\n",
            "wih with\n",
            "excersising exercising\n",
            "leaguges leagues\n",
            "greastest greatest\n",
            "exerise exercise\n",
            "doesnt doesn\n",
            "anywere anywhere\n",
            "actully actually\n",
            "simpole simple\n",
            "affectde affected\n",
            "convienet convient\n",
            "reasearch research\n",
            "adverage average\n",
            "reasearch research\n",
            "benifit benefit\n",
            "reasearch research\n",
            "insted instead\n",
            "extreamly extremely\n",
            "insted instead\n",
            "happpier happier\n",
            "aganist against\n",
            "aganist against\n",
            "thats that\n",
            "myspace space\n",
            "myspace space\n",
            "myspace space\n",
            "hookbag workbag\n",
            "myspace space\n",
            "myspace space\n",
            "bieng being\n",
            "thats that\n",
            "myspace space\n",
            "acident accident\n",
            "upgrage upgrade\n",
            "Itunes Tunes\n",
            "youtube couture\n",
            "thats that\n",
            "diccionary dictionary\n",
            "comsuming consuming\n",
            "withour without\n",
            "lesieure leisure\n",
            "imposible impossible\n",
            "keepng keeping\n",
            "buisness business\n",
            "benifit benefit\n",
            "benifits benefits\n",
            "benifit benefit\n",
            "benifit benefit\n",
            "freinds friends\n",
            "atleast least\n",
            "inportant important\n",
            "consious conscious\n",
            "obsity ossify\n",
            "abesity ability\n",
            "intresting interesting\n",
            "schedul schedule\n",
            "myspace space\n",
            "aol all\n",
            "peolple people\n",
            "sucessful successful\n",
            "avalible available\n",
            "conected connected\n",
            "innapropriate inappropriate\n",
            "rif if\n",
            "beuatiful beautiful\n",
            "diffrent different\n",
            "planin plain\n",
            "strugle struggle\n",
            "tikets tickets\n",
            "wateing watering\n",
            "incible incise\n",
            "enternet internet\n",
            "belive believe\n",
            "cholersterol cholestrol\n",
            "diabities diabetes\n",
            "ejoyable enjoyable\n",
            "jogg ogg\n",
            "homwork homework\n",
            "omputers computers\n",
            "diferent different\n",
            "acidents accidents\n",
            "sedual sexual\n",
            "presantation presentation\n",
            "acidents accidents\n",
            "practily practice\n",
            "piolot pilot\n",
            "completly completely\n",
            "completly completely\n",
            "asist assist\n",
            "insision incision\n",
            "sergons sermons\n",
            "completly completely\n",
            "sterial sternal\n",
            "benifit benefit\n",
            "acidents accidents\n",
            "garenteed guaranteed\n",
            "outsid outside\n",
            "wiil will\n",
            "excplain explain\n",
            "diffrent different\n",
            "helsp help\n",
            "oponion opinion\n",
            "alternitives alternatives\n",
            "likley likely\n",
            "haedaches headache\n",
            "excersize excessive\n",
            "exercize exercise\n",
            "youre your\n",
            "terriable terrible\n",
            "opinon opinion\n",
            "everyon everyone\n",
            "comunicate communicate\n",
            "knowlage knowledge\n",
            "diffrerent different\n",
            "anytime daytime\n",
            "diffrent different\n",
            "thse the\n",
            "comunicate communicate\n",
            "completly completely\n",
            "convienient convenient\n",
            "firsty first\n",
            "conveinient convenient\n",
            "conveinient convenient\n",
            "apreciate appreciate\n",
            "conveinent convenient\n",
            "wana want\n",
            "myspace space\n",
            "diffrent different\n",
            "opionion opinion\n",
            "typr type\n",
            "extreene extreme\n",
            "extreemly extremely\n",
            "optinions opinions\n",
            "expenencing experiencing\n",
            "abrubt abrupt\n",
            "taht that\n",
            "vurtual virtual\n",
            "probally probably\n",
            "thats that\n",
            "dargerous dangerous\n",
            "unappropriate inappropriate\n",
            "thse the\n",
            "edvance advance\n",
            "acknoowledge acknowledge\n",
            "anytime daytime\n",
            "definetly definitely\n",
            "bada bad\n",
            "bada bad\n",
            "conact contact\n",
            "myspace space\n",
            "anytime daytime\n",
            "extremly extremely\n",
            "pobably probably\n",
            "gois goes\n",
            "completly completely\n",
            "dropeed dropped\n",
            "belive believe\n",
            "wanied wanted\n",
            "thier their\n",
            "exhaused exhausted\n",
            "shuld should\n",
            "shuld should\n",
            "secon second\n",
            "wana want\n",
            "belive believe\n",
            "wana want\n",
            "abot about\n",
            "exercite exercise\n",
            "thats that\n",
            "exercite exercise\n",
            "portant portent\n",
            "exspert expert\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b468e96c1368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Spelling_Mistakes_Count\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Preprocessed_Essay\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCheck_Spelling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4159\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4160\u001b[0m         \"\"\"\n\u001b[0;32m-> 4161\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4162\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   4163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-c5802efbee33>\u001b[0m in \u001b[0;36mCheck_Spelling\u001b[0;34m(Sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspellcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# result [0][0] contains the bool value if the spelling is correct or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mspellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         '''\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/en/__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\n\u001b[1;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__contains__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__getitem__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_lazy\u001b[0;34m(self, method, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mReplaces\u001b[0m \u001b[0mlazydict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcalls\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking Punctuation Mistakes **(Incomplete)**"
      ],
      "metadata": {
        "id": "qEPOKuVJRz1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correcting Spelling Mistakes via TextBlob"
      ],
      "metadata": {
        "id": "EKmSFD923yAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Correct_Spelling(Sentence):\n",
        "  \"\"\"\n",
        "    Checks spelling of each word\n",
        "\n",
        "    Args:\n",
        "      word: Words (Tokens) of each essay \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "      \n",
        "  \"\"\"\n",
        "  Tokens = word_tokenize(Sentence)\n",
        "  newTokens = []\n",
        "  for word in Tokens:\n",
        "    word = Word(word)\n",
        "  \n",
        "    result = word.spellcheck()\n",
        "\n",
        "    # result [0][0] contains the bool value if the spelling is correct or not\n",
        "    # result [0][1] contains the confidence for the suggest correct spelling\n",
        "    \n",
        "    if word != result[0][0]:\n",
        "      if(result[0][1] > 0.9 and not(wordnet.synsets(word)) and not(\"/\" in word) and not(\"@\" in word)):\n",
        "        newTokens.append(result[0][0])\n",
        "        print(word , result[0][0])\n",
        "      else: \n",
        "        newTokens.append(word)\n",
        "    else:\n",
        "      newTokens.append(word)\n",
        "  return ' '.join(newTokens)"
      ],
      "metadata": {
        "id": "6BCrmEOO3vfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Essay_NoWhiteSpace'] = Data_Essay_01['Essay'].apply(Remove_White_Spaces)"
      ],
      "metadata": {
        "id": "P189o66CU391"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Essay_Corrected\"]  = Data_Essay_01[\"Essay_Corrected\"].map(Correct_Spelling)"
      ],
      "metadata": {
        "id": "sa2lPNPmU6Qf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e323a5f6-79ea-4dee-faa8-66f746d35cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "troble trouble\n",
            "buisness business\n",
            "myspace space\n",
            "If Of\n",
            "isnt isn\n",
            "forbidde forbidden\n",
            "troble trouble\n",
            "coordibates coordinate\n",
            "myspace space\n",
            "coordibates coordinate\n",
            "If Of\n",
            "n't not\n",
            "n't not\n",
            "appling applying\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "benifit benefit\n",
            "studdies studies\n",
            "reasearch research\n",
            "advertisments advertisements\n",
            "imposible impossible\n",
            "commucated communicated\n",
            "reasearch research\n",
            "reasearch research\n",
            "posible possible\n",
            "amagine imagine\n",
            "n't not\n",
            "buissness business\n",
            "catalouge catalogue\n",
            "castomer customer\n",
            "coustomers customers\n",
            "resturant restaurant\n",
            "convinences convinces\n",
            "reaserch research\n",
            "Computors Computers\n",
            "computors computers\n",
            "If Of\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "computors computers\n",
            "If Of\n",
            "convience convince\n",
            "n't not\n",
            "n't not\n",
            "confrence conference\n",
            "parttners partners\n",
            "conviente convient\n",
            "subssiquently subsequently\n",
            "conviente convient\n",
            "camputer computer\n",
            "posibility possibility\n",
            "everthing everything\n",
            "wouldbe would\n",
            "phisically physically\n",
            "ardthritis arthritis\n",
            "myspace space\n",
            "n't not\n",
            "embarrasing embarrassing\n",
            "'re are\n",
            "n't not\n",
            "n't not\n",
            "hand-eye handley\n",
            "n't not\n",
            "hand-eye handley\n",
            "hand-eye handley\n",
            "'re are\n",
            "hand-eye handley\n",
            "If Of\n",
            "hand-eye handley\n",
            "feild field\n",
            "hand-eye handley\n",
            "n't not\n",
            "hand-eye handley\n",
            "n't not\n",
            "hand-eye handley\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "hand-eye handley\n",
            "everyones everyone\n",
            "Ther Her\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "atleast least\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "'re are\n",
            "n't not\n",
            "obease obese\n",
            "n't not\n",
            "problum problem\n",
            "n't not\n",
            "n't not\n",
            "socioty society\n",
            "obease obese\n",
            "cumputers computers\n",
            "n't not\n",
            "n't not\n",
            "amarica america\n",
            "n't not\n",
            "n't not\n",
            "exersise exercise\n",
            "computeres computers\n",
            "mealting melting\n",
            "electricty electricity\n",
            "thjen then\n",
            "electrisity electricity\n",
            "If Of\n",
            "happining happening\n",
            "shorline shrine\n",
            "technolagy technology\n",
            "peroson person\n",
            "themselfs themselves\n",
            "conclution conclusion\n",
            "computeres computers\n",
            "amarica america\n",
            "obease obese\n",
            "reson reason\n",
            "califan halifax\n",
            "exirsis exists\n",
            "thrid third\n",
            "reson reason\n",
            "peolpe people\n",
            "exmple example\n",
            "didnt didn\n",
            "didnt didn\n",
            "adout about\n",
            "consiter consider\n",
            "thrie three\n",
            "resons reasons\n",
            "Seconde Second\n",
            "resons reasons\n",
            "postition position\n",
            "bigin begin\n",
            "insted instead\n",
            "insted instead\n",
            "incomputer computer\n",
            "becasue because\n",
            "resons reasons\n",
            "futur future\n",
            "resons reasons\n",
            "bigin begin\n",
            "insted instead\n",
            "abd and\n",
            "insted instead\n",
            "cmputer computer\n",
            "papaer paper\n",
            "resons reasons\n",
            "papaer paper\n",
            "hand-eye handley\n",
            "Thers Hers\n",
            "webcame became\n",
            "n't not\n",
            "interection interaction\n",
            "nessessity necessity\n",
            "soceity society\n",
            "usod used\n",
            "If Of\n",
            "n't not\n",
            "'re are\n",
            "Additionaly Additional\n",
            "If Of\n",
            "reasearch research\n",
            "peole people\n",
            "peole people\n",
            "scaresly scarcely\n",
            "Oviously Obviously\n",
            "foreward forward\n",
            "bility ability\n",
            "technologicall technological\n",
            "belive believe\n",
            "belive believe\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "belive believe\n",
            "n't not\n",
            "n't not\n",
            "exersise exercise\n",
            "belive believe\n",
            "n't not\n",
            "privalges privileges\n",
            "If Of\n",
            "n't not\n",
            "n't not\n",
            "addictted addicted\n",
            "n't not\n",
            "n't not\n",
            "pleanty plenty\n",
            "n't not\n",
            "belive believe\n",
            "n't not\n",
            "centainly certainly\n",
            "joggin join\n",
            "workin working\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "thats that\n",
            "n't not\n",
            "n't not\n",
            "froi from\n",
            "invanyor inventor\n",
            "leki levi\n",
            "mashine machine\n",
            "hand-eye handley\n",
            "reasearch research\n",
            "desiner designer\n",
            "Thats Hats\n",
            "communtiy community\n",
            "dieseases diseases\n",
            "neccessitys necessity\n",
            "myspace space\n",
            "youtube couture\n",
            "recestion reception\n",
            "n't not\n",
            "exersice exercise\n",
            "exersice exercise\n",
            "Thats Hats\n",
            "n't not\n",
            "exersice exercise\n",
            "n't not\n",
            "If Of\n",
            "n't not\n",
            "hand-eye handley\n",
            "foregin foreign\n",
            "hand-eye handley\n",
            "n't not\n",
            "hand-eye handley\n",
            "hand-eye handley\n",
            "tye the\n",
            "hand-eye handley\n",
            "hand-eye handley\n",
            "hand-eye handley\n",
            "If Of\n",
            "hand-eye handley\n",
            "n't not\n",
            "thats that\n",
            "n't not\n",
            "yu you\n",
            "wih with\n",
            "would't would\n",
            "n't not\n",
            "imprdove improve\n",
            "hand-eye handley\n",
            "beileve believe\n",
            "helpfull helpful\n",
            "If Of\n",
            "benifit benefit\n",
            "diffrent different\n",
            "refferances references\n",
            "adicted addicted\n",
            "beleive believe\n",
            "infermation information\n",
            "viris virus\n",
            "If Of\n",
            "infermation information\n",
            "wepons weapons\n",
            "differnt different\n",
            "goind going\n",
            "If Of\n",
            "n't not\n",
            "n't not\n",
            "whats what\n",
            "goind going\n",
            "hand-eye handley\n",
            "n't not\n",
            "computre computer\n",
            "If Of\n",
            "computre computer\n",
            "computres computers\n",
            "infront front\n",
            "computres computers\n",
            "If Of\n",
            "computres computers\n",
            "hand-eye handley\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "If Of\n",
            "n't not\n",
            "n't not\n",
            "deffinately definitely\n",
            "webkinz webbing\n",
            "usefull useful\n",
            "invatations invitations\n",
            "n't not\n",
            "n't not\n",
            "If Of\n",
            "If Of\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "thats that\n",
            "colloge college\n",
            "negitive negative\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "n't not\n",
            "If Of\n",
            "n't not\n",
            "myspace space\n",
            "If Of\n",
            "unseatled unsettled\n",
            "friesnd friend\n",
            "If Of\n",
            "If Of\n",
            "understandd understand\n",
            "hand-eye handley\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-13fe4baceccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Essay_Corrected\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Essay_Corrected\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCorrect_Spelling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4159\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4160\u001b[0m         \"\"\"\n\u001b[0;32m-> 4161\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4162\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   4163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-d2f202d79d7a>\u001b[0m in \u001b[0;36mCorrect_Spelling\u001b[0;34m(Sentence)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspellcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# result [0][0] contains the bool value if the spelling is correct or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mspellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         '''\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/en/__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\n\u001b[1;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__contains__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__getitem__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_lazy\u001b[0;34m(self, method, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correcting Spelling Mistakes via LanguageTool"
      ],
      "metadata": {
        "id": "ndVmwOgqVIfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Spelling_Error_Correct(essays):\n",
        "    matches = tool.check(essays)\n",
        "    is_bad_rule = lambda rule: rule.category == 'GRAMMAR'\n",
        "    matches = [rule for rule in matches if not is_bad_rule(rule)]\n",
        "    # print(matches[0].category)\n",
        "    language_tool_python.utils.correct(essays, matches)   # to correct it\n",
        "    return essays"
      ],
      "metadata": {
        "id": "dLb4MnckVMVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Essay_SpellingCorrected_LT'] = Data_Essay_01['Essay_NoWhiteSpace'].apply(Spelling_Error_Correct)"
      ],
      "metadata": {
        "id": "i3m8RRJJVMMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Punctuation Mistakes"
      ],
      "metadata": {
        "id": "N2BerbAju6gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification , pipeline"
      ],
      "metadata": {
        "id": "9oOhfQJHdo0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('oliverguhr/fullstop-punctuation-multilang-large')\n",
        "model = AutoModelForTokenClassification.from_pretrained('oliverguhr/fullstop-punctuation-multilang-large')\n",
        "pun = pipeline('ner' , model = model , tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "Wa2ndqK-T2p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = pun(text)\n",
        "\n",
        "Updated_string = ''\n",
        "\n",
        "for output in tags:\n",
        "  result = output['word'].replace('▁' , ' ') + output['entity'].replace('0', '')\n",
        "  Updated_string += result\n",
        "\n",
        "Updated_string"
      ],
      "metadata": {
        "id": "BZ4O_fCzUwHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Number Of Punctuations"
      ],
      "metadata": {
        "id": "0GCMQ41VQbyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Count_Punctuations(Essay):\n",
        "  \"\"\"\n",
        "    Counts Punctuations used in an Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int,int,int,int,int\n",
        "      \n",
        "  \"\"\"\n",
        "  count_fullstops = 0\n",
        "  count_exclamation = 0\n",
        "  count_comma = 0\n",
        "  count_hyphens = 0\n",
        "  count_questionmark = 0\n",
        "\n",
        "  tokens = word_tokenize(Essay)\n",
        "\n",
        "  for word in tokens:\n",
        "    if word == \".\":\n",
        "      count_fullstops += 1\n",
        "    elif word == \"!\":\n",
        "      count_exclamation += 1\n",
        "    elif word == \"?\":\n",
        "      count_questionmark += 1\n",
        "    elif word == \",\":\n",
        "      count_comma += 1\n",
        "    elif word == \"-\":\n",
        "      count_hyphens += 1\n",
        "\n",
        "  return count_fullstops , count_exclamation , count_comma , count_questionmark , count_hyphens"
      ],
      "metadata": {
        "id": "c8GDruwFQbYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Count_Fullstops\"] , Data_Essay_01[\"Count_Exclamation\"] , Data_Essay_01[\"Count_Comma\"] , Data_Essay_01[\"Count_Questionmark\"] , Data_Essay_01[\"Count_Hyphens\"] = zip(*Data_Essay_01[\"Essay\"].map(Count_Punctuations))\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "5ZTt9va_Vsuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking Capitalization Mistakes"
      ],
      "metadata": {
        "id": "3mZ9RfrmQwpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Check_Capitalization(Essay):\n",
        "  \"\"\"\n",
        "    Checks capitalization in each sentence of an essay\n",
        "\n",
        "    Args:\n",
        "    Essay: Words (Tokens) of each essay \n",
        "\n",
        "    Returns: \n",
        "    int\n",
        "\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "\n",
        "  words = word_tokenize(Essay)\n",
        "  temp = []\n",
        "  # Checking Capital Letters in start of every sentence & start of every quote\n",
        "  for i in range(len(words) - 1):\n",
        "    \n",
        "    if (i == 0):\n",
        "      if words[i] != words[i].title():\n",
        "        count = count + 1\n",
        "    elif words[i] == '.' or words[i] == '\"':\n",
        "      if not(\"@\" in words[i+1]):\n",
        "        match = words[i+1]\n",
        "        if match != words[i+1].title():\n",
        "          count = count + 1\n",
        "      else:\n",
        "        i = i + 2\n",
        "    else:\n",
        "      for character in words[i]:\n",
        "        if character.isupper():\n",
        "          temp.append(words[i])\n",
        "          count = count + 1\n",
        "    \n",
        "  # Checking if all proper nouns are capital or not\n",
        "  tagged_sent = pos_tag(words)\n",
        "\n",
        "  for word,pos in tagged_sent:\n",
        "    if(pos == 'NNP'):\n",
        "      if (word in temp):\n",
        "        count = count - 1\n",
        "        temp.remove(word)\n",
        "      if word != word.title():\n",
        "        count = count + 1\n",
        "\n",
        "  return count"
      ],
      "metadata": {
        "id": "IP6L-cA7SArH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Capitalization_Errors\"] = Data_Essay_01[\"Essay\"].apply(Check_Capitalization)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "AOma5U8XSBBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Grammar Error Detection**"
      ],
      "metadata": {
        "id": "kZEkV3-ruDBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# reference = result.text.split()\n",
        "\n",
        "# candidate = 'Dear local newspaper, @CAPS1 best friend, @LOCATION2, was once a nerd with no hand-eye coordination, @CAPS2, he started to use a computer and now he has better hand-eye coordination than me.'.split()\n",
        "# print('BLEU score -> {}'.format(sentence_bleu(reference, candidate )))"
      ],
      "metadata": {
        "id": "dxk_61AvuGQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = Data_Essay_01[['Essay_SpellingCorrected_LT', 'Sent_Count']]\n",
        "# df1['Essay_Spelling_Corrected_LT'] = df1['Essay_SpellingCorrected_LT'].apply(Remove_White_Spaces)   # to avoid whitespace error\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "3CrujkhOuQ_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "abc3d319-6a41-4f7c-8343-d92558ebf5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Essay_SpellingCorrected_LT  Sent_Count  \\\n",
              "0  Dear local newspaper, I think effects computer...          16   \n",
              "1  Dear @CAPS1 @CAPS2, I believe that using compu...          20   \n",
              "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...          14   \n",
              "3  Dear Local Newspaper, @CAPS1 I have found that...          27   \n",
              "4  Dear @LOCATION1, I know having computers has a...          30   \n",
              "\n",
              "                         Essay_Spelling_Corrected_LT  \n",
              "0  Dear local newspaper, I think effects computer...  \n",
              "1  Dear @CAPS1 @CAPS2, I believe that using compu...  \n",
              "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...  \n",
              "3  Dear Local Newspaper, @CAPS1 I have found that...  \n",
              "4  Dear @LOCATION1, I know having computers has a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dbb9bd3a-2db0-4af7-a27d-9df338d18ecb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Essay_SpellingCorrected_LT</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Essay_Spelling_Corrected_LT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>16</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>20</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>14</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>27</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>30</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbb9bd3a-2db0-4af7-a27d-9df338d18ecb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dbb9bd3a-2db0-4af7-a27d-9df338d18ecb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dbb9bd3a-2db0-4af7-a27d-9df338d18ecb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Grammar_Errors(essays):\n",
        "    \n",
        "    matches = tool.check(essays)\n",
        "    is_bad_rule = lambda rule: rule.category == 'GRAMMAR'\n",
        "    matches = [rule for rule in matches if is_bad_rule(rule)]\n",
        "    # print(matches[0].category)\n",
        "    errors = []\n",
        "    #language_tool_python.utils.correct(text, matches)   # to correct it\n",
        "    for i in range(0, len(matches)):\n",
        "      errors.append(matches[i].ruleId)  # or category of the error (Misc, Whitespace, Typography)\n",
        "    return len(matches), errors"
      ],
      "metadata": {
        "id": "2kdFSdAfuSWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Grammar_Errors(\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\")"
      ],
      "metadata": {
        "id": "VEgaOfy5WE9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a257b2-1513-4146-ae0a-217aed544b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, ['CAUSE_BECAUSE', 'BE_VBP_IN'])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data_Essay_01['Grammar_Errors'], Data_Essay_01['Grammar_Error_List'] = zip(*df1_copy['Essay'].map(grammar_errors))\n",
        "Data_Essay_01['Grammar_Error_Count'], Data_Essay_01['Grammar_Error_List'] = zip(*df1['Essay_SpellingCorrected_LT'].map(Grammar_Errors))"
      ],
      "metadata": {
        "id": "UCNsXW7MuToX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01.columns"
      ],
      "metadata": {
        "id": "qby9k_DduUsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bf19bb-67b0-47fd-da1e-761fc57f168a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Essay', 'Rater_1 Score', 'Rater_2 Score', 'Total Score',\n",
              "       'Sent_Count', 'Word_Count', 'Char_Count', 'Avg_Word_Count',\n",
              "       'Preprocessed_Essay', 'Verb_Count', 'Noun_Count', 'Adj_Count',\n",
              "       'Conj_Count', 'Adverb_Count', 'pNoun_Count', 'Essay_Corrected',\n",
              "       'Essay_Correct_LT', 'Grammar_Errors', 'Grammar_Error_List',\n",
              "       'Essay_SpellingCorrected_LT', 'Grammar_Error_Count',\n",
              "       'Essay_NoWhiteSpace'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = Data_Essay_01['Grammar_Error_List'].explode().value_counts()\n",
        "out"
      ],
      "metadata": {
        "id": "4KttcpfcuV6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cfdca15-c854-4206-f782-05761ec4cfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HE_VERB_AGR             228\n",
              "A_NNS                   139\n",
              "ITS_TO_IT_S              96\n",
              "MD_BASEFORM              91\n",
              "BEEN_PART_AGREEMENT      86\n",
              "                       ... \n",
              "SIMPLE_TO_USE_HYPHEN      1\n",
              "GOOD_GOOF                 1\n",
              "DO_YOU_FASCINATED         1\n",
              "DOES_YOU                  1\n",
              "SHUTDOWN                  1\n",
              "Name: Grammar_Error_List, Length: 324, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.to_csv('GrammarErrors.csv')"
      ],
      "metadata": {
        "id": "Mj7LNN5JuW2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = Data_Essay_01[['Sent_Count', 'Word_Count', 'Char_Count', 'Avg_Word_Count','Verb_Count', 'Noun_Count', 'Adj_Count',\n",
        "       'Conj_Count', 'Adverb_Count', 'pNoun_Count', 'Grammar_Error_Count', 'Grammar_Error_List']]\n",
        "features.to_csv(\"EssaySet01_Features.csv\")     "
      ],
      "metadata": {
        "id": "e5kQjwH_XH00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AD0l-C_LOUAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lexical Sophistication**"
      ],
      "metadata": {
        "id": "ttyjcisFxNHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install taaled\n",
        "#RESOURCES FOR LEXICAL SOPHISTICATION \n",
        "#https://eli-data-mining-group.github.io/Pitt-ELI-Corpus/publications/Naismith_2019.pdf\n",
        "#https://pypi.org/project/taaled/\n",
        "#https://github.com/LCR-ADS-Lab/pylats"
      ],
      "metadata": {
        "id": "PT5e5b5axQSg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "3686242f-3ae4-41f7-c6f8-6212a1c00cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-7193a176c34a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install taaled\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVX_WRUzxQNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LL9lGScHxQFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Content**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Latent Semantic Analysis (LSA)\n"
      ],
      "metadata": {
        "id": "iNcrlo78xaLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Latent Semantic Analysis (LSA)**\n",
        "\n",
        "Content analysis generally implies only a high-level semantic analysis and comparison with source text and graded essays"
      ],
      "metadata": {
        "id": "0Mv-IDjowJET"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gs2o1P6xesp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantic**\n",
        "Semantic metrics assess the correctness of content connotation"
      ],
      "metadata": {
        "id": "MSiuF2JzyBWX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cExW4ZuiyWyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantic Coherence & Consistency**"
      ],
      "metadata": {
        "id": "jQ--G0SrwJqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2-_rC8mwdfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Connectivity**"
      ],
      "metadata": {
        "id": "nEFmhVcBwKAq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYamIfnRwuqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Readibility Scores**\n",
        "\n",
        "We section will cover:\n",
        "1.   Flesch Reading Ease\n",
        "2.   Flesch-Kincaid Grade Level\n",
        "3.   Gunning Fog Index\n",
        "4.   Dale Chall Readability Formula\n",
        "5.   Shannon Entropy\n",
        "6.   Simpson's Index"
      ],
      "metadata": {
        "id": "-uvdm3joSeUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Flesch_Reading_East_Score(scount,NoOfsentences,total_Words):\n",
        " return (206.835-1.015*(total_Words/float(NoOfsentences))-84.6*(scount / float(total_Words)))"
      ],
      "metadata": {
        "id": "YQm0D0h4Se1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in Data_Essay_01.iterrows():\n",
        "  Data_Essay_01['Flesch_Reading_Score'][index]=Flesch_Reading_East_Score(row[\"Syllable_Count\"],row[\"Sent_Count\"],row[\"Word_Count\"])"
      ],
      "metadata": {
        "id": "JxZIddenStzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0d9vjiulC7aL",
        "pmuQWvabiYC-",
        "9jYVbje0Ex_m",
        "6VY92aM5E00E",
        "LCGMHv_yE29j",
        "eSWTBDDpizP2",
        "SSNhc_FfKHld",
        "iUMnW4Qm4Vws",
        "qEPOKuVJRz1-",
        "0GCMQ41VQbyC",
        "ttyjcisFxNHZ",
        "iNcrlo78xaLJ",
        "0Mv-IDjowJET",
        "MSiuF2JzyBWX",
        "jQ--G0SrwJqQ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}