{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Packages**"
      ],
      "metadata": {
        "id": "t5wewbNqz8cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpP642wc8kXg",
        "outputId": "228df552-c1a1-440f-96b4-22ec85e0879f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.13.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 46.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.13.0 textstat-0.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language-tool-python) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language-tool-python) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (1.24.3)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n",
        "!pip install sentencepiece  \n",
        "!pip install transformers\n",
        "!pip install textstat\n",
        "!pip install language-tool-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart Runtime after installing "
      ],
      "metadata": {
        "id": "oFW9msHtUg-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Packages**"
      ],
      "metadata": {
        "id": "-CLigcFJ0ABg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzNPxq8X8bYG",
        "outputId": "0d734914-4d3d-429c-f1da-a9ea86ceae66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Downloading LanguageTool 5.7: 100%|██████████| 225M/225M [00:13<00:00, 17.3MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmptjddw5d3.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-5.7.zip to /root/.cache/language_tool_python.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk \n",
        "import spacy\n",
        "import textstat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn \n",
        "from textblob import Word\n",
        "import matplotlib.pyplot as plt\n",
        "import language_tool_python\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tool = language_tool_python.LanguageTool('en-US')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading Data From Google Drive**"
      ],
      "metadata": {
        "id": "B9QmvZ2KGsPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I7JhmdSg8swA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "5382f1fd-decb-42e8-b130-8fb6ba167673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                              Essay  Rater_1 Score  \\\n",
              "0   1  Dear local newspaper, I think effects computer...            4.0   \n",
              "1   2  Dear @CAPS1 @CAPS2, I believe that using compu...            5.0   \n",
              "2   3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...            4.0   \n",
              "3   4  Dear Local Newspaper, @CAPS1 I have found that...            5.0   \n",
              "4   5  Dear @LOCATION1, I know having computers has a...            4.0   \n",
              "\n",
              "   Rater_2 Score  Total Score  \n",
              "0            4.0          8.0  \n",
              "1            4.0          9.0  \n",
              "2            3.0          7.0  \n",
              "3            5.0         10.0  \n",
              "4            4.0          8.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0290a61-da4b-4fe6-b2d8-2a1529466831\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0290a61-da4b-4fe6-b2d8-2a1529466831')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0290a61-da4b-4fe6-b2d8-2a1529466831 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0290a61-da4b-4fe6-b2d8-2a1529466831');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "Data_Essay_01 = pd.read_csv(\"/content/drive/MyDrive/IntelliTech-DataSet/EssaySet01.csv\")\n",
        "Data_Essay_01.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4CyChIa3hm3"
      },
      "source": [
        "# **Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Essay Pre Processing**"
      ],
      "metadata": {
        "id": "0d9vjiulC7aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Remove_NER(Essay):\n",
        "  \"\"\"\n",
        "    Removes Named Entity Recognition (NER) from each essay\n",
        "\n",
        "    Args:\n",
        "      Sentence: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "\n",
        "  \"\"\"\n",
        "  token = Essay.split()\n",
        "  newTokens = []\n",
        "  # print(token)\n",
        "  for i in range(len(token)):\n",
        "    if \"@\" in token[i]:\n",
        "      if \",\" in token[i]:\n",
        "        newTokens.append(\",\")\n",
        "      elif \".\" in token[i]:\n",
        "        newTokens.append(\".\")\n",
        "      elif \"!\" in token[i]:\n",
        "        newTokens.append(\"!\")\n",
        "    else:\n",
        "      newTokens.append(token[i])\n",
        "\n",
        "  return \" \".join(newTokens)\n",
        "  # return ' '.join (word for word in Essay.split(' ') if not word.startswith('@'))\n",
        "\n",
        "def Remove_Punctuations(sentence):\n",
        "  \"\"\"\n",
        "    Removes punctuations from text\n",
        "    Args:\n",
        "      sentence: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  punctuations = '''!()-[]{};:\"\\,/'<>.?@#$%^&*_~'''\n",
        "  newSentence = \"\"\n",
        "  for word in sentence:\n",
        "      if (word in punctuations):\n",
        "          newSentence = newSentence + \" \"\n",
        "      else: \n",
        "          newSentence = newSentence + word\n",
        "  return newSentence\n",
        "\n",
        "def LowerCase_Words(Essay):\n",
        "  \"\"\"\n",
        "    Lower case all the words in an essay\n",
        "\n",
        "    Args:\n",
        "      Sentence: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  return re.sub('[0-9]+','', Essay).lower() \n",
        "\n",
        "def Tokenize_Essay(Essay):\n",
        "    \"\"\"\n",
        "      Create Tokens of each Essay\n",
        "\n",
        "      Args:\n",
        "        Essay: Essay of each student\n",
        "      \n",
        "      Returns: \n",
        "        String\n",
        "    \"\"\"\n",
        "    Preprocessed = Remove_Punctuations(Essay)\n",
        "    return \" \".join(word_tokenize(Preprocessed))\n",
        "\n",
        "def Remove_White_Spaces(Essay):\n",
        "  \"\"\"\n",
        "    Removes Extra White Spaces\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  return \" \".join(Essay.split())\n",
        "\n",
        "def Remove_Special_Characters(Essay):\n",
        "  \"\"\"\n",
        "    Removes Special Characters from Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  new_text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", Essay)\n",
        "  return new_text"
      ],
      "metadata": {
        "id": "6qLIKn4LC7-_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Basic Count Features**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Sentences per Essay\n",
        "*   Counting Words per Essay\n",
        "*   Counting Characters per Essay\n",
        "*   Average Words per Essay\n",
        "*   Counting Syllables\n"
      ],
      "metadata": {
        "id": "pmuQWvabiYC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Sentences per Essay"
      ],
      "metadata": {
        "id": "9jYVbje0Ex_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nV3ttAkblI57"
      },
      "outputs": [],
      "source": [
        "def Sentence_Count(Essay):\n",
        "    \"\"\"\n",
        "    Counts sentences in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "    sentence_no = nltk.sent_tokenize(Essay)\n",
        "    return len(sentence_no)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Sent_Count'] = Data_Essay_01['Essay'].apply(Sentence_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "hQzk8MtddUbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "38ae7d6d-3b5e-489d-f07b-719c2cdd15d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID                                              Essay  Rater_1 Score  \\\n",
              "123  124  I think that the computers are good to be used...            4.0   \n",
              "\n",
              "     Rater_2 Score  Total Score  Sent_Count  \n",
              "123            3.0          7.0           9  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d52626e-577d-4cec-bf97-1878b1b12fc5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>124</td>\n",
              "      <td>I think that the computers are good to be used...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d52626e-577d-4cec-bf97-1878b1b12fc5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d52626e-577d-4cec-bf97-1878b1b12fc5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d52626e-577d-4cec-bf97-1878b1b12fc5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Words per Essay"
      ],
      "metadata": {
        "id": "6VY92aM5E00E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** These word count are more than the original count coz of nltk tokenization. Punctations are treated as seperate words.\n"
      ],
      "metadata": {
        "id": "YxGrgzWIFzPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Word_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts words in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int  \n",
        "  \"\"\" \n",
        "  word_no = nltk.word_tokenize(Essay)\n",
        "  return len(word_no)"
      ],
      "metadata": {
        "id": "7OPVrqpAdLec"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Word_Count'] = Data_Essay_01['Essay'].apply(Word_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "8kb3-p5edQzI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "5ed82385-fa73-47e0-9898-bbfc47a73389"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID                                              Essay  Rater_1 Score  \\\n",
              "904  907  Dear newspaper and readers of @LOCATION1, I am...            5.0   \n",
              "\n",
              "     Rater_2 Score  Total Score  Sent_Count  Word_Count  \n",
              "904            4.0          9.0          19         409  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2ba58e89-fd8c-4ea2-a0a8-387ba02fede7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Word_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>907</td>\n",
              "      <td>Dear newspaper and readers of @LOCATION1, I am...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>19</td>\n",
              "      <td>409</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ba58e89-fd8c-4ea2-a0a8-387ba02fede7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2ba58e89-fd8c-4ea2-a0a8-387ba02fede7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2ba58e89-fd8c-4ea2-a0a8-387ba02fede7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Characters per Essay"
      ],
      "metadata": {
        "id": "LCGMHv_yE29j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Char_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts characters in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "  return len([character for character in Essay])"
      ],
      "metadata": {
        "id": "ELmS3Tt-f-nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Char_Count'] = Data_Essay_01['Essay'].apply(Char_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "AchO9qj9dMzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Average Word Length of Essay"
      ],
      "metadata": {
        "id": "eSWTBDDpizP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Avg_Word_Count(Essay):\n",
        "  \"\"\"\n",
        "    Calculates Average Word Count In An Essay Set\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      float\n",
        "      \n",
        "  \"\"\"\n",
        "  word_list = nltk.word_tokenize(Essay)\n",
        "  total = sum(map(len, word_list))/len(word_list)\n",
        "  return total"
      ],
      "metadata": {
        "id": "HujBMTSuXxxt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Avg_Word_Count'] = Data_Essay_01['Essay'].apply(Avg_Word_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "D5A2-UkIdX4Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "53c4a008-a853-4558-8c28-86643e87c5b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ID                                              Essay  Rater_1 Score  \\\n",
              "718  721  Dear local newspaper, it has came to @CAPS2 at...            5.0   \n",
              "\n",
              "     Rater_2 Score  Total Score  Sent_Count  Word_Count  Avg_Word_Count  \n",
              "718            4.0          9.0          21         433        4.205543  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-163634e6-093b-443c-a6e5-acaf4b052d7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Word_Count</th>\n",
              "      <th>Avg_Word_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>721</td>\n",
              "      <td>Dear local newspaper, it has came to @CAPS2 at...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>21</td>\n",
              "      <td>433</td>\n",
              "      <td>4.205543</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-163634e6-093b-443c-a6e5-acaf4b052d7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-163634e6-093b-443c-a6e5-acaf4b052d7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-163634e6-093b-443c-a6e5-acaf4b052d7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Syllables"
      ],
      "metadata": {
        "id": "SSNhc_FfKHld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Syllable_Count(text):\n",
        "  return textstat.syllable_count(text, lang='en_US')"
      ],
      "metadata": {
        "id": "1ihBnNT6KGnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Preprocessed_Essay\"].apply(LowerCase_Words)\n",
        "Data_Essay_01[\"Syllable_Count\"]=Data_Essay_01[\"Preprocessed_Essay\"].apply(Syllable_Count)\n",
        "Data_Essay_01[\"Syllable_Count\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "QexLRdtK4J1P",
        "outputId": "97724fc1-3f0b-4578-b1ca-06bc568005f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Essay'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0a6e24dc4686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Preprocessed_Essay\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLowerCase_Words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Syllable_Count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Preprocessed_Essay\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSyllable_Count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Syllable_Count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Essay'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parts Of Speech Counts**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Nouns per Essay\n",
        "*   Counting Adjectives per Essay\n",
        "*   Counting Proper Nouns per Essay\n",
        "*   Counting Adverbs per Essay\n",
        "*   Counting Conjunctions per Essay"
      ],
      "metadata": {
        "id": "iUMnW4Qm4Vws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing NERs, Punctuations and Lower Casing"
      ],
      "metadata": {
        "id": "A0qjHkwRH7FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Preprocessed_Essay'] = Data_Essay_01['Essay'].apply(Remove_NER)\n",
        "Data_Essay_01['Preprocessed_Essay'] = Data_Essay_01['Preprocessed_Essay'].apply(Tokenize_Essay)\n",
        "Data_Essay_01.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "mxsK-yprDcfV",
        "outputId": "e77cc466-98fd-446c-c8a7-f0ffd83d0ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                              Essay  Rater_1 Score  \\\n",
              "0   1  Dear local newspaper, I think effects computer...            4.0   \n",
              "1   2  Dear @CAPS1 @CAPS2, I believe that using compu...            5.0   \n",
              "2   3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...            4.0   \n",
              "3   4  Dear Local Newspaper, @CAPS1 I have found that...            5.0   \n",
              "4   5  Dear @LOCATION1, I know having computers has a...            4.0   \n",
              "\n",
              "   Rater_2 Score  Total Score  Sent_Count  Word_Count  Char_Count  \\\n",
              "0            4.0          8.0          16         386        1875   \n",
              "1            4.0          9.0          20         464        2288   \n",
              "2            3.0          7.0          14         313        1541   \n",
              "3            5.0         10.0          27         611        3165   \n",
              "4            4.0          8.0          30         517        2569   \n",
              "\n",
              "   Avg_Word_Count                                 Preprocessed_Essay  \n",
              "0        3.984456  Dear local newspaper I think effects computers...  \n",
              "1        4.030172  Dear I believe that using computers will benef...  \n",
              "2        4.035144  Dear More and more people use computers but no...  \n",
              "3        4.328969  Dear Local Newspaper I have found that many ex...  \n",
              "4        4.071567  Dear I know having computers has a positive ef...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-641cc7f8-b951-4646-96f8-355c6cec27e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Word_Count</th>\n",
              "      <th>Char_Count</th>\n",
              "      <th>Avg_Word_Count</th>\n",
              "      <th>Preprocessed_Essay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16</td>\n",
              "      <td>386</td>\n",
              "      <td>1875</td>\n",
              "      <td>3.984456</td>\n",
              "      <td>Dear local newspaper I think effects computers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>20</td>\n",
              "      <td>464</td>\n",
              "      <td>2288</td>\n",
              "      <td>4.030172</td>\n",
              "      <td>Dear I believe that using computers will benef...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>14</td>\n",
              "      <td>313</td>\n",
              "      <td>1541</td>\n",
              "      <td>4.035144</td>\n",
              "      <td>Dear More and more people use computers but no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>27</td>\n",
              "      <td>611</td>\n",
              "      <td>3165</td>\n",
              "      <td>4.328969</td>\n",
              "      <td>Dear Local Newspaper I have found that many ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>30</td>\n",
              "      <td>517</td>\n",
              "      <td>2569</td>\n",
              "      <td>4.071567</td>\n",
              "      <td>Dear I know having computers has a positive ef...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-641cc7f8-b951-4646-96f8-355c6cec27e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-641cc7f8-b951-4646-96f8-355c6cec27e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-641cc7f8-b951-4646-96f8-355c6cec27e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Pos_Tag_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts Parts of Speech in an Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int,int,int,int,int,int    \n",
        "  \"\"\"\n",
        "  tagged_doc = nlp(Essay)\n",
        "\n",
        "  adj_count=0\n",
        "  verb_count=0\n",
        "  noun_count=0\n",
        "  pNoun_count=0\n",
        "  adverb_count=0\n",
        "  conj_count=0\n",
        "\n",
        "  for token in tagged_doc:\n",
        "\n",
        "    if(token.pos_ == 'ADJ'):\n",
        "      adj_count+=1\n",
        "    \n",
        "    elif(token.pos_ =='NOUN'):\n",
        "      noun_count+=1\n",
        "\n",
        "    elif (token.pos_ =='PRON'):\n",
        "      pNoun_count+=1\n",
        "\n",
        "    elif (token.pos_ =='VERB'):\n",
        "      verb_count+=1\n",
        "\n",
        "    elif (token.pos_ =='ADV'):\n",
        "      adverb_count+=1\n",
        "    \n",
        "    elif(token.pos_=='CCONJ'):\n",
        "      conj_count+=1\n",
        "\n",
        "  return verb_count,noun_count, adj_count, conj_count, adverb_count,pNoun_count"
      ],
      "metadata": {
        "id": "26j3wRKD4X54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Verb_Count'], Data_Essay_01['Noun_Count'], Data_Essay_01['Adj_Count'], Data_Essay_01['Conj_Count'], Data_Essay_01['Adverb_Count'], Data_Essay_01['pNoun_Count']=zip(*Data_Essay_01[\"Preprocessed_Essay\"].map(Pos_Tag_Count))\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "W-WSN5Ce85Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating Writing Attributes**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Style\n",
        "*   Content\n",
        "*   Semantic\n",
        "*   Semantic Coherence & Consistency \n",
        "*   Connectivity\n",
        "*   Readibility Scores\n"
      ],
      "metadata": {
        "id": "puuRk5EHw8jJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Style**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Mechanics\n",
        "*   Grammar\n",
        "*   Lexical Sophistication\n",
        "\n"
      ],
      "metadata": {
        "id": "Id4wN-9AxITZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mechanics**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Spelling Mistakes\n",
        "*   Correcting Spelling Mistakes\n",
        "*   Checking Punctuations\n",
        "*   Counting Punctuations\n",
        "*   Checking Capitalization\n",
        "\n"
      ],
      "metadata": {
        "id": "I6sEA-fCCfmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Spelling Mistakes"
      ],
      "metadata": {
        "id": "2NgCS1PQZaIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW5kZQP-W3CE"
      },
      "outputs": [],
      "source": [
        "def Check_Spelling(Sentence):\n",
        "  \"\"\"\n",
        "    Checks spelling of each word\n",
        "\n",
        "    Args:\n",
        "      word: Words (Tokens) of each essay \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  Sentence = word_tokenize(Sentence)\n",
        "  for word in Sentence:\n",
        "    word = Word(word)\n",
        "  \n",
        "    result = word.spellcheck()\n",
        "\n",
        "    # result [0][0] contains the bool value if the spelling is correct or not\n",
        "    # result [0][1] contains the confidence for the suggest correct spelling\n",
        "\n",
        "    if word != result[0][0]:\n",
        "      if(result[0][1] > 0.9 and not(wordnet.synsets(word)) and not(\"/\" in word) and not (word == \"If\" or word == \"if\")):\n",
        "        count = count + 1\n",
        "\n",
        "  return count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Preprocessed_Essay\"] = Data_Essay_01[\"Essay\"].apply(Remove_NER)"
      ],
      "metadata": {
        "id": "Lhbvn0kuFGnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Spelling_Mistakes_Count\"]  = Data_Essay_01[\"Preprocessed_Essay\"].map(Check_Spelling)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "u5UO3OIYE-8p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "29c9745c-ef25-4225-d787-c7fad1ad7537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b468e96c1368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Spelling_Mistakes_Count\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Preprocessed_Essay\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCheck_Spelling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mData_Essay_01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4159\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4160\u001b[0m         \"\"\"\n\u001b[0;32m-> 4161\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4162\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   4163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-859ec355bc86>\u001b[0m in \u001b[0;36mCheck_Spelling\u001b[0;34m(Sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspellcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# result [0][0] contains the bool value if the spelling is correct or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mspellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         '''\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/en/__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\n\u001b[1;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_edit1\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSpelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALPHA\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSpelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALPHA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m         )\n\u001b[1;32m   1368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSpelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALPHA\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSpelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALPHA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m         )\n\u001b[1;32m   1368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correcting Spelling Mistakes via LanguageTool"
      ],
      "metadata": {
        "id": "ndVmwOgqVIfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Spelling_Error_Correct(essays):\n",
        "    matches = tool.check(essays)\n",
        "    is_bad_rule = lambda rule: rule.category == 'GRAMMAR'\n",
        "    matches = [rule for rule in matches if not is_bad_rule(rule)]\n",
        "    # print(matches[0].category)\n",
        "    language_tool_python.utils.correct(essays, matches)   # to correct it\n",
        "    return essays"
      ],
      "metadata": {
        "id": "dLb4MnckVMVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Essay_SpellingCorrected_LT'] = Data_Essay_01['Essay_NoWhiteSpace'].apply(Spelling_Error_Correct)"
      ],
      "metadata": {
        "id": "i3m8RRJJVMMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking Punctuation Mistakes **(Incomplete)**"
      ],
      "metadata": {
        "id": "qEPOKuVJRz1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Punctuation Mistakes"
      ],
      "metadata": {
        "id": "N2BerbAju6gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification , pipeline"
      ],
      "metadata": {
        "id": "9oOhfQJHdo0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('oliverguhr/fullstop-punctuation-multilang-large')\n",
        "model = AutoModelForTokenClassification.from_pretrained('oliverguhr/fullstop-punctuation-multilang-large')\n",
        "pun = pipeline('ner' , model = model , tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "Wa2ndqK-T2p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = pun(text)\n",
        "\n",
        "Updated_string = ''\n",
        "\n",
        "for output in tags:\n",
        "  result = output['word'].replace('▁' , ' ') + output['entity'].replace('0', '')\n",
        "  Updated_string += result\n",
        "\n",
        "Updated_string"
      ],
      "metadata": {
        "id": "BZ4O_fCzUwHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Punctuation_Errors(essays):\n",
        "    matches = tool.check(essays)\n",
        "    is_bad_rule = lambda rule: rule.category == 'PUNCTUATION'\n",
        "    matches = [rule for rule in matches if is_bad_rule(rule)]\n",
        "    return len(matches)"
      ],
      "metadata": {
        "id": "cjcop4Kfrkb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Without_White_Spaces\"] = Data_Essay_01[\"Essay\"].apply(Remove_White_Spaces)"
      ],
      "metadata": {
        "id": "_TNsg7Eerlz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Punctuation_Errors\"] = Data_Essay_01[\"Without_White_Spaces\"].apply(Punctuation_Errors)"
      ],
      "metadata": {
        "id": "SJtdDornsMoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Number Of Punctuations"
      ],
      "metadata": {
        "id": "0GCMQ41VQbyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Count_Punctuations(Essay):\n",
        "  \"\"\"\n",
        "    Counts Punctuations used in an Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int,int,int,int,int\n",
        "      \n",
        "  \"\"\"\n",
        "  count_fullstops = 0\n",
        "  count_exclamation = 0\n",
        "  count_comma = 0\n",
        "  count_hyphens = 0\n",
        "  count_questionmark = 0\n",
        "\n",
        "  tokens = word_tokenize(Essay)\n",
        "\n",
        "  for word in tokens:\n",
        "    if word == \".\":\n",
        "      count_fullstops += 1\n",
        "    elif word == \"!\":\n",
        "      count_exclamation += 1\n",
        "    elif word == \"?\":\n",
        "      count_questionmark += 1\n",
        "    elif word == \",\":\n",
        "      count_comma += 1\n",
        "    elif word == \"-\":\n",
        "      count_hyphens += 1\n",
        "\n",
        "  return count_fullstops , count_exclamation , count_comma , count_questionmark , count_hyphens"
      ],
      "metadata": {
        "id": "c8GDruwFQbYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Count_Fullstops\"] , Data_Essay_01[\"Count_Exclamation\"] , Data_Essay_01[\"Count_Comma\"] , Data_Essay_01[\"Count_Questionmark\"] , Data_Essay_01[\"Count_Hyphens\"] = zip(*Data_Essay_01[\"Essay\"].map(Count_Punctuations))\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "5ZTt9va_Vsuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking Capitalization Mistakes"
      ],
      "metadata": {
        "id": "3mZ9RfrmQwpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Check_Capitalization(Essay):\n",
        "  \"\"\"\n",
        "    Checks capitalization in each sentence of an essay\n",
        "\n",
        "    Args:\n",
        "    Essay: Words (Tokens) of each essay \n",
        "\n",
        "    Returns: \n",
        "    int\n",
        "\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "\n",
        "  words = Essay.split()\n",
        "  alreadyCounted_Words = []\n",
        "  \n",
        "  for i in range(len(words) - 1):\n",
        "    if (i == 0):                                                    # Checking Capital Letter at the start of Sentence\n",
        "      if words[i] != words[i].title():\n",
        "        alreadyCounted_Words.append(words[i])\n",
        "        count = count + 1\n",
        "    elif \"@\" in words[i]:\n",
        "      continue\n",
        "    elif words[i] == '.' or words[i] == '\"':                         # Checking Capital Letters in start of every sentence & start of every quote\n",
        "      match = words[i+1]\n",
        "      if match != words[i+1].title():\n",
        "        alreadyCounted_Words.append(words[i])\n",
        "        count = count + 1\n",
        "        i = i + 1\n",
        "    \n",
        "    # Check if capital in middle \n",
        "    \n",
        "  # Checking if all proper nouns are capital or not\n",
        "\n",
        "  tagged_sent = nlp(Essay)\n",
        "\n",
        "  for i in range(len(tagged_sent)):\n",
        "    if tagged_sent[i].pos_ == \"PROPN\":\n",
        "      # print(tagged_sent[i]) \n",
        "      word = tagged_sent[i].text \n",
        "      if word in alreadyCounted_Words:\n",
        "        alreadyCounted_Words.remove(word)\n",
        "      elif word != word.title():\n",
        "        count = count + 1\n",
        "\n",
        "  return count"
      ],
      "metadata": {
        "id": "IP6L-cA7SArH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Preprocessed_Essay\"] =  Data_Essay_01[\"Essay\"].apply(Remove_NER)\n",
        "Data_Essay_01[\"Capitalization_Errors\"] = Data_Essay_01[\"Preprocessed_Essay\"].apply(Check_Capitalization)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "AOma5U8XSBBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01.to_csv(\"EssaySet01_Features_Updated.csv\")     "
      ],
      "metadata": {
        "id": "hEo6xSNCqXMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Grammar Error Detection**"
      ],
      "metadata": {
        "id": "kZEkV3-ruDBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# reference = result.text.split()\n",
        "\n",
        "# candidate = 'Dear local newspaper, @CAPS1 best friend, @LOCATION2, was once a nerd with no hand-eye coordination, @CAPS2, he started to use a computer and now he has better hand-eye coordination than me.'.split()\n",
        "# print('BLEU score -> {}'.format(sentence_bleu(reference, candidate )))"
      ],
      "metadata": {
        "id": "dxk_61AvuGQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = Data_Essay_01[['Essay_SpellingCorrected_LT', 'Sent_Count']]\n",
        "# df1['Essay_Spelling_Corrected_LT'] = df1['Essay_SpellingCorrected_LT'].apply(Remove_White_Spaces)   # to avoid whitespace error\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "3CrujkhOuQ_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Grammar_Errors(essays):\n",
        "    \n",
        "    matches = tool.check(essays)\n",
        "    is_bad_rule = lambda rule: rule.category == 'GRAMMAR'\n",
        "    matches = [rule for rule in matches if is_bad_rule(rule)]\n",
        "    # print(matches[0].category)\n",
        "    errors = []\n",
        "    #language_tool_python.utils.correct(text, matches)   # to correct it\n",
        "    for i in range(0, len(matches)):\n",
        "      errors.append(matches[i].ruleId)  # or category of the error (Misc, Whitespace, Typography)\n",
        "    return len(matches), errors"
      ],
      "metadata": {
        "id": "2kdFSdAfuSWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Grammar_Errors(\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\")"
      ],
      "metadata": {
        "id": "VEgaOfy5WE9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data_Essay_01['Grammar_Errors'], Data_Essay_01['Grammar_Error_List'] = zip(*df1_copy['Essay'].map(grammar_errors))\n",
        "Data_Essay_01['Grammar_Error_Count'], Data_Essay_01['Grammar_Error_List'] = zip(*df1['Essay_SpellingCorrected_LT'].map(Grammar_Errors))"
      ],
      "metadata": {
        "id": "UCNsXW7MuToX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01.columns"
      ],
      "metadata": {
        "id": "qby9k_DduUsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = Data_Essay_01['Grammar_Error_List'].explode().value_counts()\n",
        "out"
      ],
      "metadata": {
        "id": "4KttcpfcuV6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.to_csv('GrammarErrors.csv')"
      ],
      "metadata": {
        "id": "Mj7LNN5JuW2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = Data_Essay_01[['Sent_Count', 'Word_Count', 'Char_Count', 'Avg_Word_Count','Verb_Count', 'Noun_Count', 'Adj_Count',\n",
        "       'Conj_Count', 'Adverb_Count', 'pNoun_Count', 'Grammar_Error_Count', 'Grammar_Error_List']]\n",
        "features.to_csv(\"EssaySet01_Features.csv\")     "
      ],
      "metadata": {
        "id": "e5kQjwH_XH00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AD0l-C_LOUAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lexical Sophistication**"
      ],
      "metadata": {
        "id": "ttyjcisFxNHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install taaled\n",
        "#RESOURCES FOR LEXICAL SOPHISTICATION \n",
        "#https://eli-data-mining-group.github.io/Pitt-ELI-Corpus/publications/Naismith_2019.pdf\n",
        "#https://pypi.org/project/taaled/\n",
        "#https://github.com/LCR-ADS-Lab/pylats"
      ],
      "metadata": {
        "id": "PT5e5b5axQSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVX_WRUzxQNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LL9lGScHxQFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Content**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Latent Semantic Analysis (LSA)\n"
      ],
      "metadata": {
        "id": "iNcrlo78xaLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Latent Semantic Analysis (LSA)**\n",
        "\n",
        "Content analysis generally implies only a high-level semantic analysis and comparison with source text and graded essays"
      ],
      "metadata": {
        "id": "0Mv-IDjowJET"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gs2o1P6xesp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantic**\n",
        "Semantic metrics assess the correctness of content connotation"
      ],
      "metadata": {
        "id": "MSiuF2JzyBWX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cExW4ZuiyWyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantic Coherence & Consistency**"
      ],
      "metadata": {
        "id": "jQ--G0SrwJqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2-_rC8mwdfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Connectivity**"
      ],
      "metadata": {
        "id": "nEFmhVcBwKAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step01: Making a temp dataframe for sepearte preprocessing of connectors\n",
        "df_connectors = Data_Essay_01[['Essay', 'Sent_Count']]\n",
        "df_connectors['Preprocessed_Essay_For_Connectors'] = df_connectors['Essay'].apply(LowerCase_Words)  # for ease in matching\n",
        "df_connectors['Preprocessed_Essay_For_Connectors'] = df_connectors['Preprocessed_Essay_For_Connectors'].apply(Remove_NER) # no impact on connectors count, removing it to save computational time.\n",
        "df_connectors['Preprocessed_Essay_For_Connectors'] = df_connectors['Preprocessed_Essay_For_Connectors'].apply(Remove_White_Spaces) # for ease in ngrams\n",
        "df_connectors.head()\n"
      ],
      "metadata": {
        "id": "CYamIfnRwuqD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "eaddaa5b-5976-4964-a15c-ebd91c5a802e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Essay  Sent_Count  \\\n",
              "0  Dear local newspaper, I think effects computer...          16   \n",
              "1  Dear @CAPS1 @CAPS2, I believe that using compu...          20   \n",
              "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...          14   \n",
              "3  Dear Local Newspaper, @CAPS1 I have found that...          27   \n",
              "4  Dear @LOCATION1, I know having computers has a...          30   \n",
              "\n",
              "                   Preprocessed_Essay_For_Connectors  \n",
              "0  dear local newspaper, i think effects computer...  \n",
              "1  dear , i believe that using computers will ben...  \n",
              "2  dear, more and more people use computers, but ...  \n",
              "3  dear local newspaper, i have found that many e...  \n",
              "4  dear , i know having computers has a positive ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-409fec58-2137-44c8-9677-02849af1acd5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Essay</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Preprocessed_Essay_For_Connectors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>16</td>\n",
              "      <td>dear local newspaper, i think effects computer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>20</td>\n",
              "      <td>dear , i believe that using computers will ben...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>14</td>\n",
              "      <td>dear, more and more people use computers, but ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>27</td>\n",
              "      <td>dear local newspaper, i have found that many e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>30</td>\n",
              "      <td>dear , i know having computers has a positive ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-409fec58-2137-44c8-9677-02849af1acd5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-409fec58-2137-44c8-9677-02849af1acd5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-409fec58-2137-44c8-9677-02849af1acd5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step02: Generating n_grams of each essay. \n",
        "def generate_N_grams(essay,ngram=1):\n",
        "  words=[word for word in essay.split(\" \")]  \n",
        "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
        "  ngram_list=[' '.join(ngram) for ngram in temp]\n",
        "  return ngram_list"
      ],
      "metadata": {
        "id": "uOXKXKvNzJfn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_connectors['Unigrams'] = df_connectors['Preprocessed_Essay_For_Connectors'].apply(generate_N_grams, ngram =1)\n",
        "df_connectors['Bigrams'] = df_connectors['Preprocessed_Essay_For_Connectors'].apply(generate_N_grams, ngram =2)\n",
        "df_connectors['Trigrams'] = df_connectors['Preprocessed_Essay_For_Connectors'].apply(generate_N_grams, ngram =3)\n",
        "df_connectors['Four-grams'] = df_connectors['Preprocessed_Essay_For_Connectors'].apply(generate_N_grams, ngram =4)\n",
        "df_connectors['Five-grams'] = df_connectors['Preprocessed_Essay_For_Connectors'].apply(generate_N_grams, ngram =5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTilf11_0__e",
        "outputId": "57951e24-17c3-4571-983d-bd13c9bdbd09"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_connectors['Four-grams'][17]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M277gjUM29p_",
        "outputId": "b76b90d3-d825-47d6-928a-85765df0802d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dear local newspaper, i',\n",
              " 'local newspaper, i must',\n",
              " 'newspaper, i must admit',\n",
              " 'i must admit that',\n",
              " 'must admit that the',\n",
              " 'admit that the experts',\n",
              " 'that the experts are',\n",
              " 'the experts are centainly',\n",
              " 'experts are centainly right',\n",
              " 'are centainly right cause',\n",
              " 'centainly right cause i',\n",
              " 'right cause i know',\n",
              " 'cause i know people',\n",
              " 'i know people that',\n",
              " 'know people that stay',\n",
              " 'people that stay on',\n",
              " 'that stay on the',\n",
              " 'stay on the computer',\n",
              " 'on the computer all',\n",
              " 'the computer all day',\n",
              " 'computer all day and',\n",
              " 'all day and that’s',\n",
              " 'day and that’s really',\n",
              " 'and that’s really not',\n",
              " 'that’s really not good',\n",
              " 'really not good if',\n",
              " 'not good if you',\n",
              " 'good if you ask',\n",
              " 'if you ask me.',\n",
              " 'you ask me. instead',\n",
              " 'ask me. instead of',\n",
              " 'me. instead of being',\n",
              " 'instead of being on',\n",
              " 'of being on computers',\n",
              " 'being on computers learning',\n",
              " 'on computers learning about',\n",
              " 'computers learning about faraway',\n",
              " 'learning about faraway places,',\n",
              " 'about faraway places, and',\n",
              " 'faraway places, and talking',\n",
              " 'places, and talking to',\n",
              " 'and talking to other',\n",
              " 'talking to other people,',\n",
              " 'to other people, you',\n",
              " 'other people, you should',\n",
              " 'people, you should go',\n",
              " 'you should go exercising,',\n",
              " 'should go exercising, you',\n",
              " 'go exercising, you should',\n",
              " 'exercising, you should relax,',\n",
              " 'you should relax, and',\n",
              " 'should relax, and you',\n",
              " 'relax, and you should',\n",
              " 'and you should interact',\n",
              " 'you should interact with',\n",
              " 'should interact with family',\n",
              " 'interact with family and',\n",
              " 'with family and friends.',\n",
              " 'family and friends. exercising',\n",
              " 'and friends. exercising is',\n",
              " 'friends. exercising is much',\n",
              " 'exercising is much more',\n",
              " 'is much more better',\n",
              " 'much more better than',\n",
              " 'more better than being',\n",
              " 'better than being on',\n",
              " 'than being on a',\n",
              " 'being on a computer,',\n",
              " 'on a computer, and',\n",
              " 'a computer, and i',\n",
              " 'computer, and i say',\n",
              " 'and i say that',\n",
              " 'i say that to',\n",
              " 'say that to say',\n",
              " 'that to say this.',\n",
              " 'to say this. when',\n",
              " 'say this. when your',\n",
              " 'this. when your on',\n",
              " 'when your on a',\n",
              " 'your on a computer',\n",
              " 'on a computer just',\n",
              " 'a computer just sitting',\n",
              " 'computer just sitting there',\n",
              " 'just sitting there playing',\n",
              " 'sitting there playing online',\n",
              " 'there playing online games',\n",
              " 'playing online games and',\n",
              " 'online games and checking',\n",
              " 'games and checking other',\n",
              " 'and checking other things',\n",
              " 'checking other things out,',\n",
              " 'other things out, or',\n",
              " 'things out, or whatever',\n",
              " 'out, or whatever the',\n",
              " 'or whatever the case',\n",
              " 'whatever the case be,',\n",
              " 'the case be, you',\n",
              " 'case be, you get',\n",
              " 'be, you get board,',\n",
              " 'you get board, and',\n",
              " 'get board, and hungry,',\n",
              " 'board, and hungry, and',\n",
              " 'and hungry, and i',\n",
              " 'hungry, and i know',\n",
              " 'and i know theres',\n",
              " 'i know theres some',\n",
              " 'know theres some snacks',\n",
              " 'theres some snacks around',\n",
              " 'some snacks around you',\n",
              " 'snacks around you while',\n",
              " 'around you while your',\n",
              " 'you while your doing',\n",
              " 'while your doing whatever',\n",
              " 'your doing whatever your',\n",
              " 'doing whatever your doing.',\n",
              " 'whatever your doing. instead',\n",
              " 'your doing. instead of',\n",
              " 'doing. instead of being',\n",
              " 'instead of being on',\n",
              " 'of being on a',\n",
              " 'being on a computer',\n",
              " 'on a computer for',\n",
              " 'a computer for or',\n",
              " 'computer for or hours',\n",
              " 'for or hours or',\n",
              " 'or hours or maybe',\n",
              " 'hours or maybe even',\n",
              " 'or maybe even all',\n",
              " 'maybe even all day',\n",
              " 'even all day getting',\n",
              " 'all day getting fat.',\n",
              " 'day getting fat. there',\n",
              " 'getting fat. there are',\n",
              " 'fat. there are many',\n",
              " 'there are many ways',\n",
              " 'are many ways you',\n",
              " 'many ways you can',\n",
              " 'ways you can exercise.',\n",
              " 'you can exercise. you',\n",
              " 'can exercise. you can',\n",
              " 'exercise. you can take',\n",
              " 'you can take walks,',\n",
              " 'can take walks, you',\n",
              " 'take walks, you can',\n",
              " 'walks, you can go',\n",
              " 'you can go joggin,',\n",
              " 'can go joggin, you',\n",
              " 'go joggin, you could',\n",
              " 'joggin, you could play',\n",
              " 'you could play a',\n",
              " 'could play a sport,',\n",
              " 'play a sport, i',\n",
              " 'a sport, i mean',\n",
              " 'sport, i mean there',\n",
              " 'i mean there are',\n",
              " 'mean there are a',\n",
              " 'there are a lot',\n",
              " 'are a lot of',\n",
              " 'a lot of things',\n",
              " 'lot of things you',\n",
              " 'of things you can',\n",
              " 'things you can do',\n",
              " 'you can do when',\n",
              " 'can do when your',\n",
              " 'do when your not',\n",
              " 'when your not on',\n",
              " 'your not on a',\n",
              " 'not on a computer.',\n",
              " 'on a computer. i',\n",
              " 'a computer. i know',\n",
              " 'computer. i know some',\n",
              " 'i know some people',\n",
              " 'know some people that',\n",
              " 'some people that could',\n",
              " 'people that could be',\n",
              " 'that could be workin',\n",
              " 'could be workin all',\n",
              " 'be workin all day',\n",
              " 'workin all day and',\n",
              " 'all day and still',\n",
              " 'day and still come',\n",
              " 'and still come home',\n",
              " 'still come home to',\n",
              " 'come home to there',\n",
              " 'home to there computer.',\n",
              " 'to there computer. rest',\n",
              " 'there computer. rest is',\n",
              " 'computer. rest is good',\n",
              " 'rest is good also.',\n",
              " 'is good also. you',\n",
              " \"good also. you don't\",\n",
              " \"also. you don't always\",\n",
              " \"you don't always have\",\n",
              " \"don't always have to\",\n",
              " 'always have to jump',\n",
              " 'have to jump on',\n",
              " 'to jump on your',\n",
              " 'jump on your computer,',\n",
              " 'on your computer, you',\n",
              " 'your computer, you could',\n",
              " 'computer, you could just',\n",
              " 'you could just relax',\n",
              " 'could just relax watch',\n",
              " 'just relax watch a',\n",
              " 'relax watch a movie,',\n",
              " 'watch a movie, eat',\n",
              " 'a movie, eat some',\n",
              " 'movie, eat some pop',\n",
              " 'eat some pop corn',\n",
              " 'some pop corn and',\n",
              " 'pop corn and just',\n",
              " 'corn and just enjoy',\n",
              " 'and just enjoy yourself,',\n",
              " 'just enjoy yourself, you',\n",
              " 'enjoy yourself, you could',\n",
              " 'yourself, you could even',\n",
              " 'you could even take',\n",
              " 'could even take a',\n",
              " 'even take a nap',\n",
              " 'take a nap for',\n",
              " 'a nap for a',\n",
              " 'nap for a little',\n",
              " 'for a little while',\n",
              " 'a little while or',\n",
              " 'little while or something',\n",
              " 'while or something but',\n",
              " 'or something but you',\n",
              " \"something but you don't\",\n",
              " \"but you don't always\",\n",
              " \"you don't always have\",\n",
              " \"don't always have to\",\n",
              " 'always have to get',\n",
              " 'have to get straight',\n",
              " 'to get straight on',\n",
              " 'get straight on your',\n",
              " 'straight on your computer',\n",
              " 'on your computer if',\n",
              " 'your computer if you',\n",
              " 'computer if you really',\n",
              " \"if you really don't\",\n",
              " \"you really don't have\",\n",
              " \"really don't have to\",\n",
              " \"don't have to use\",\n",
              " 'have to use it.',\n",
              " 'to use it. the',\n",
              " 'use it. the way',\n",
              " 'it. the way i',\n",
              " 'the way i see',\n",
              " 'way i see it',\n",
              " 'i see it is',\n",
              " 'see it is family,',\n",
              " 'it is family, and',\n",
              " 'is family, and computers',\n",
              " 'family, and computers are',\n",
              " 'and computers are always',\n",
              " 'computers are always going',\n",
              " 'are always going to',\n",
              " 'always going to be',\n",
              " 'going to be there',\n",
              " 'to be there but',\n",
              " 'be there but at',\n",
              " 'there but at the',\n",
              " 'but at the same',\n",
              " 'at the same time',\n",
              " 'the same time which',\n",
              " 'same time which one',\n",
              " 'time which one is',\n",
              " 'which one is more',\n",
              " 'one is more important.',\n",
              " 'is more important. friends',\n",
              " \"more important. friends aren't\",\n",
              " \"important. friends aren't always\",\n",
              " \"friends aren't always dependable\",\n",
              " \"aren't always dependable so\",\n",
              " 'always dependable so you',\n",
              " 'dependable so you really',\n",
              " \"so you really don't\",\n",
              " \"you really don't need\",\n",
              " \"really don't need them\",\n",
              " \"don't need them around\",\n",
              " 'need them around only',\n",
              " 'them around only a',\n",
              " 'around only a selected',\n",
              " 'only a selected few,',\n",
              " 'a selected few, but',\n",
              " 'selected few, but thats',\n",
              " 'few, but thats not',\n",
              " 'but thats not the',\n",
              " 'thats not the point.',\n",
              " 'not the point. both',\n",
              " 'the point. both family,',\n",
              " 'point. both family, and',\n",
              " 'both family, and your',\n",
              " 'family, and your true',\n",
              " 'and your true friends',\n",
              " 'your true friends have',\n",
              " 'true friends have feelings,',\n",
              " 'friends have feelings, and',\n",
              " 'have feelings, and if',\n",
              " 'feelings, and if your',\n",
              " 'and if your not',\n",
              " 'if your not spending',\n",
              " 'your not spending time',\n",
              " 'not spending time with',\n",
              " 'spending time with them',\n",
              " 'time with them over',\n",
              " 'with them over a',\n",
              " 'them over a computer',\n",
              " 'over a computer that',\n",
              " 'a computer that your',\n",
              " 'computer that your gonna',\n",
              " 'that your gonna have',\n",
              " 'your gonna have when',\n",
              " 'gonna have when you',\n",
              " 'have when you leave,',\n",
              " 'when you leave, and',\n",
              " 'you leave, and come',\n",
              " 'leave, and come back',\n",
              " 'and come back home',\n",
              " 'come back home too',\n",
              " 'back home too something',\n",
              " \"home too something isn't\",\n",
              " \"too something isn't right.\",\n",
              " \"something isn't right. tomorrow\",\n",
              " \"isn't right. tomorrow isn't\",\n",
              " \"right. tomorrow isn't promised,\",\n",
              " \"tomorrow isn't promised, we\",\n",
              " \"isn't promised, we live\",\n",
              " 'promised, we live for',\n",
              " 'we live for today.',\n",
              " 'live for today. so',\n",
              " 'for today. so enjoy',\n",
              " 'today. so enjoy life',\n",
              " 'so enjoy life while',\n",
              " 'enjoy life while you',\n",
              " 'life while you got',\n",
              " 'while you got the',\n",
              " 'you got the chance,',\n",
              " 'got the chance, and',\n",
              " 'the chance, and spend',\n",
              " 'chance, and spend time',\n",
              " 'and spend time doing',\n",
              " 'spend time doing something',\n",
              " 'time doing something active,',\n",
              " 'doing something active, with',\n",
              " 'something active, with family',\n",
              " 'active, with family and',\n",
              " 'with family and friends,',\n",
              " 'family and friends, and',\n",
              " 'and friends, and if',\n",
              " 'friends, and if your',\n",
              " 'and if your really',\n",
              " 'if your really into',\n",
              " 'your really into computers,',\n",
              " 'really into computers, try',\n",
              " 'into computers, try to',\n",
              " 'computers, try to balance',\n",
              " 'try to balance your',\n",
              " 'to balance your time',\n",
              " 'balance your time out.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step03: Loading the file of connectors.\n",
        "filename = \"/content/drive/MyDrive/IntelliTech-DataSet/Connectors_Words.txt\"\n",
        "with open(filename) as file:\n",
        "    defined_connectors = [line.rstrip() for line in file]\n",
        "\n",
        "defined_connectors = set(defined_connectors)  #remove duplicate connectors\n",
        "print(len(defined_connectors))"
      ],
      "metadata": {
        "id": "2SEow0YI6Od6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3c9228-50d3-4f5f-d2d9-0171c40d97a9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 04: Make a connector detection function\n",
        "\n",
        "def connectors_detection(unigrams_list, bigram_list, trigram_list, four_grams_list, five_grams_list):\n",
        "  connectors_detected = []\n",
        "  for token in unigrams_list:\n",
        "    if token in defined_connectors:\n",
        "      connectors_detected.append(token)\n",
        "  for token in bigram_list:\n",
        "    if token in defined_connectors:\n",
        "      connectors_detected.append(token)\n",
        "  for token in trigram_list:\n",
        "    if token in defined_connectors:\n",
        "      connectors_detected.append(token)\n",
        "  for token in four_grams_list:\n",
        "    if token in defined_connectors:\n",
        "      connectors_detected.append(token)\n",
        "  for token in five_grams_list:\n",
        "    if token in defined_connectors:\n",
        "      connectors_detected.append(token)\n",
        "  \n",
        "  #use SET connectors_detected for tf-idf work, unique connectors\n",
        "  return set(connectors_detected)\n",
        "  #return len(connectors_detected)"
      ],
      "metadata": {
        "id": "yAEl8Pfkp-r7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_connectors['Connectors'] = df_connectors.apply(lambda row: connectors_detection(row['Unigrams'], row['Bigrams'],row['Trigrams'],row['Four-grams'],row['Five-grams']),axis = 1)"
      ],
      "metadata": {
        "id": "8Gqa3wOajKwv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_connectors.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "5VHYoXw6dhVU",
        "outputId": "a7ea17d0-f4bc-44d0-86df-9bf5c7871c60"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Essay  Sent_Count  \\\n",
              "0  Dear local newspaper, I think effects computer...          16   \n",
              "1  Dear @CAPS1 @CAPS2, I believe that using compu...          20   \n",
              "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...          14   \n",
              "3  Dear Local Newspaper, @CAPS1 I have found that...          27   \n",
              "4  Dear @LOCATION1, I know having computers has a...          30   \n",
              "\n",
              "                   Preprocessed_Essay_For_Connectors  \\\n",
              "0  dear local newspaper, i think effects computer...   \n",
              "1  dear , i believe that using computers will ben...   \n",
              "2  dear, more and more people use computers, but ...   \n",
              "3  dear local newspaper, i have found that many e...   \n",
              "4  dear , i know having computers has a positive ...   \n",
              "\n",
              "                                            Unigrams  \\\n",
              "0  [dear, local, newspaper,, i, think, effects, c...   \n",
              "1  [dear, ,, i, believe, that, using, computers, ...   \n",
              "2  [dear,, more, and, more, people, use, computer...   \n",
              "3  [dear, local, newspaper,, i, have, found, that...   \n",
              "4  [dear, ,, i, know, having, computers, has, a, ...   \n",
              "\n",
              "                                             Bigrams  \\\n",
              "0  [dear local, local newspaper,, newspaper, i, i...   \n",
              "1  [dear ,, , i, i believe, believe that, that us...   \n",
              "2  [dear, more, more and, and more, more people, ...   \n",
              "3  [dear local, local newspaper,, newspaper, i, i...   \n",
              "4  [dear ,, , i, i know, know having, having comp...   \n",
              "\n",
              "                                            Trigrams  \\\n",
              "0  [dear local newspaper,, local newspaper, i, ne...   \n",
              "1  [dear , i, , i believe, i believe that, believ...   \n",
              "2  [dear, more and, more and more, and more peopl...   \n",
              "3  [dear local newspaper,, local newspaper, i, ne...   \n",
              "4  [dear , i, , i know, i know having, know havin...   \n",
              "\n",
              "                                          Four-grams  \\\n",
              "0  [dear local newspaper, i, local newspaper, i t...   \n",
              "1  [dear , i believe, , i believe that, i believe...   \n",
              "2  [dear, more and more, more and more people, an...   \n",
              "3  [dear local newspaper, i, local newspaper, i h...   \n",
              "4  [dear , i know, , i know having, i know having...   \n",
              "\n",
              "                                          Five-grams  Number_of_Connectors  \\\n",
              "0  [dear local newspaper, i think, local newspape...                    41   \n",
              "1  [dear , i believe that, , i believe that using...                    52   \n",
              "2  [dear, more and more people, more and more peo...                    35   \n",
              "3  [dear local newspaper, i have, local newspaper...                    60   \n",
              "4  [dear , i know having, , i know having compute...                    58   \n",
              "\n",
              "                                          Connectors  \n",
              "0  {but, now, and, about, while, or, to, then, wh...  \n",
              "1  {but, when, like, now, and, over, not only, fo...  \n",
              "2  {but, instead, and, for, to, or, then, up, whe...  \n",
              "3  {after, and, that is, suddenly, when, as, for,...  \n",
              "4  {between, and, so that, as a result, when, as,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1bb2914-514b-45e0-afaf-3a5902bf0744\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Essay</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Preprocessed_Essay_For_Connectors</th>\n",
              "      <th>Unigrams</th>\n",
              "      <th>Bigrams</th>\n",
              "      <th>Trigrams</th>\n",
              "      <th>Four-grams</th>\n",
              "      <th>Five-grams</th>\n",
              "      <th>Number_of_Connectors</th>\n",
              "      <th>Connectors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>16</td>\n",
              "      <td>dear local newspaper, i think effects computer...</td>\n",
              "      <td>[dear, local, newspaper,, i, think, effects, c...</td>\n",
              "      <td>[dear local, local newspaper,, newspaper, i, i...</td>\n",
              "      <td>[dear local newspaper,, local newspaper, i, ne...</td>\n",
              "      <td>[dear local newspaper, i, local newspaper, i t...</td>\n",
              "      <td>[dear local newspaper, i think, local newspape...</td>\n",
              "      <td>41</td>\n",
              "      <td>{but, now, and, about, while, or, to, then, wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>20</td>\n",
              "      <td>dear , i believe that using computers will ben...</td>\n",
              "      <td>[dear, ,, i, believe, that, using, computers, ...</td>\n",
              "      <td>[dear ,, , i, i believe, believe that, that us...</td>\n",
              "      <td>[dear , i, , i believe, i believe that, believ...</td>\n",
              "      <td>[dear , i believe, , i believe that, i believe...</td>\n",
              "      <td>[dear , i believe that, , i believe that using...</td>\n",
              "      <td>52</td>\n",
              "      <td>{but, when, like, now, and, over, not only, fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>14</td>\n",
              "      <td>dear, more and more people use computers, but ...</td>\n",
              "      <td>[dear,, more, and, more, people, use, computer...</td>\n",
              "      <td>[dear, more, more and, and more, more people, ...</td>\n",
              "      <td>[dear, more and, more and more, and more peopl...</td>\n",
              "      <td>[dear, more and more, more and more people, an...</td>\n",
              "      <td>[dear, more and more people, more and more peo...</td>\n",
              "      <td>35</td>\n",
              "      <td>{but, instead, and, for, to, or, then, up, whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>27</td>\n",
              "      <td>dear local newspaper, i have found that many e...</td>\n",
              "      <td>[dear, local, newspaper,, i, have, found, that...</td>\n",
              "      <td>[dear local, local newspaper,, newspaper, i, i...</td>\n",
              "      <td>[dear local newspaper,, local newspaper, i, ne...</td>\n",
              "      <td>[dear local newspaper, i, local newspaper, i h...</td>\n",
              "      <td>[dear local newspaper, i have, local newspaper...</td>\n",
              "      <td>60</td>\n",
              "      <td>{after, and, that is, suddenly, when, as, for,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>30</td>\n",
              "      <td>dear , i know having computers has a positive ...</td>\n",
              "      <td>[dear, ,, i, know, having, computers, has, a, ...</td>\n",
              "      <td>[dear ,, , i, i know, know having, having comp...</td>\n",
              "      <td>[dear , i, , i know, i know having, know havin...</td>\n",
              "      <td>[dear , i know, , i know having, i know having...</td>\n",
              "      <td>[dear , i know having, , i know having compute...</td>\n",
              "      <td>58</td>\n",
              "      <td>{between, and, so that, as a result, when, as,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1bb2914-514b-45e0-afaf-3a5902bf0744')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b1bb2914-514b-45e0-afaf-3a5902bf0744 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b1bb2914-514b-45e0-afaf-3a5902bf0744');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The minimum value that the index may generate is 0. There is no specified maximum limit on the index. Zero (0) denotes no connector at all, while an index close to 2 shows that there are many effective connectors in (almost) every sentence. The connectivity index is calculated for each essay."
      ],
      "metadata": {
        "id": "yMuuhUlY6kA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizing connectors via TF-IDF\n",
        "\n",
        "def TF(essay,connectors):\n",
        "  freq = 0\n",
        "  for word in essay.split(\" \"):\n",
        "    if word in connectors:\n",
        "      freq = freq + 1\n",
        "  return freq\n",
        "\n",
        "def IDF(essay,connector):\n",
        "  doc = 0;\n",
        "  sent_list = nltk.tokenize.sent_tokenize(essay)\n",
        "  for i in range(0, len(sent_list)):\n",
        "    if connector in sent_list[i]:\n",
        "      doc = doc + 1\n",
        "  return 1+ np.log(len(sent_list) / (doc))\n",
        "\n",
        "def TF_IDF(essay, connector):\n",
        "  tf = TF(essay,connector)\n",
        "  idf = IDF(essay,connector)\n",
        "  tf_idf = tf * idf\n",
        "  return tf_idf\n",
        "\n",
        "def connectivity_index(essay, connectors,num_sent):\n",
        "  weights = []\n",
        "  for word in connectors:\n",
        "    score = TF_IDF(essay,word)\n",
        "    weights.append(score)\n",
        "  return float(sum(weights) / num_sent)\n",
        "\n",
        "Data_Essay_01['Connectivity_Index'] = df_connectors.apply(lambda row: connectivity_index(row['Preprocessed_Essay_For_Connectors'], row['Connectors'], row['Sent_Count']), axis=1)\n"
      ],
      "metadata": {
        "id": "oDWgOjY6hOIf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_connectors.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "acyPdFEd4X4M",
        "outputId": "64e313b5-1669-43e5-cf08-d6db227f1bd1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Essay  Sent_Count  \\\n",
              "0  Dear local newspaper, I think effects computer...          16   \n",
              "1  Dear @CAPS1 @CAPS2, I believe that using compu...          20   \n",
              "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...          14   \n",
              "3  Dear Local Newspaper, @CAPS1 I have found that...          27   \n",
              "4  Dear @LOCATION1, I know having computers has a...          30   \n",
              "\n",
              "                   Preprocessed_Essay_For_Connectors  \\\n",
              "0  dear local newspaper, i think effects computer...   \n",
              "1  dear , i believe that using computers will ben...   \n",
              "2  dear, more and more people use computers, but ...   \n",
              "3  dear local newspaper, i have found that many e...   \n",
              "4  dear , i know having computers has a positive ...   \n",
              "\n",
              "                                            Unigrams  \\\n",
              "0  [dear, local, newspaper,, i, think, effects, c...   \n",
              "1  [dear, ,, i, believe, that, using, computers, ...   \n",
              "2  [dear,, more, and, more, people, use, computer...   \n",
              "3  [dear, local, newspaper,, i, have, found, that...   \n",
              "4  [dear, ,, i, know, having, computers, has, a, ...   \n",
              "\n",
              "                                             Bigrams  \\\n",
              "0  [dear local, local newspaper,, newspaper, i, i...   \n",
              "1  [dear ,, , i, i believe, believe that, that us...   \n",
              "2  [dear, more, more and, and more, more people, ...   \n",
              "3  [dear local, local newspaper,, newspaper, i, i...   \n",
              "4  [dear ,, , i, i know, know having, having comp...   \n",
              "\n",
              "                                            Trigrams  \\\n",
              "0  [dear local newspaper,, local newspaper, i, ne...   \n",
              "1  [dear , i, , i believe, i believe that, believ...   \n",
              "2  [dear, more and, more and more, and more peopl...   \n",
              "3  [dear local newspaper,, local newspaper, i, ne...   \n",
              "4  [dear , i, , i know, i know having, know havin...   \n",
              "\n",
              "                                          Four-grams  \\\n",
              "0  [dear local newspaper, i, local newspaper, i t...   \n",
              "1  [dear , i believe, , i believe that, i believe...   \n",
              "2  [dear, more and more, more and more people, an...   \n",
              "3  [dear local newspaper, i, local newspaper, i h...   \n",
              "4  [dear , i know, , i know having, i know having...   \n",
              "\n",
              "                                          Five-grams  Number_of_Connectors  \\\n",
              "0  [dear local newspaper, i think, local newspape...                    41   \n",
              "1  [dear , i believe that, , i believe that using...                    52   \n",
              "2  [dear, more and more people, more and more peo...                    35   \n",
              "3  [dear local newspaper, i have, local newspaper...                    60   \n",
              "4  [dear , i know having, , i know having compute...                    58   \n",
              "\n",
              "                                          Connectors  Connectivity_Index  \n",
              "0  {but, now, and, about, while, or, to, then, wh...           24.552787  \n",
              "1  {but, when, like, now, and, over, not only, fo...           25.344232  \n",
              "2  {but, instead, and, for, to, or, then, up, whe...           14.662850  \n",
              "3  {after, and, that is, suddenly, when, as, for,...           32.408323  \n",
              "4  {between, and, so that, as a result, when, as,...           27.824804  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-914659e4-13c4-41ba-83e5-a48610aea57c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Essay</th>\n",
              "      <th>Sent_Count</th>\n",
              "      <th>Preprocessed_Essay_For_Connectors</th>\n",
              "      <th>Unigrams</th>\n",
              "      <th>Bigrams</th>\n",
              "      <th>Trigrams</th>\n",
              "      <th>Four-grams</th>\n",
              "      <th>Five-grams</th>\n",
              "      <th>Number_of_Connectors</th>\n",
              "      <th>Connectors</th>\n",
              "      <th>Connectivity_Index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>16</td>\n",
              "      <td>dear local newspaper, i think effects computer...</td>\n",
              "      <td>[dear, local, newspaper,, i, think, effects, c...</td>\n",
              "      <td>[dear local, local newspaper,, newspaper, i, i...</td>\n",
              "      <td>[dear local newspaper,, local newspaper, i, ne...</td>\n",
              "      <td>[dear local newspaper, i, local newspaper, i t...</td>\n",
              "      <td>[dear local newspaper, i think, local newspape...</td>\n",
              "      <td>41</td>\n",
              "      <td>{but, now, and, about, while, or, to, then, wh...</td>\n",
              "      <td>24.552787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>20</td>\n",
              "      <td>dear , i believe that using computers will ben...</td>\n",
              "      <td>[dear, ,, i, believe, that, using, computers, ...</td>\n",
              "      <td>[dear ,, , i, i believe, believe that, that us...</td>\n",
              "      <td>[dear , i, , i believe, i believe that, believ...</td>\n",
              "      <td>[dear , i believe, , i believe that, i believe...</td>\n",
              "      <td>[dear , i believe that, , i believe that using...</td>\n",
              "      <td>52</td>\n",
              "      <td>{but, when, like, now, and, over, not only, fo...</td>\n",
              "      <td>25.344232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>14</td>\n",
              "      <td>dear, more and more people use computers, but ...</td>\n",
              "      <td>[dear,, more, and, more, people, use, computer...</td>\n",
              "      <td>[dear, more, more and, and more, more people, ...</td>\n",
              "      <td>[dear, more and, more and more, and more peopl...</td>\n",
              "      <td>[dear, more and more, more and more people, an...</td>\n",
              "      <td>[dear, more and more people, more and more peo...</td>\n",
              "      <td>35</td>\n",
              "      <td>{but, instead, and, for, to, or, then, up, whe...</td>\n",
              "      <td>14.662850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>27</td>\n",
              "      <td>dear local newspaper, i have found that many e...</td>\n",
              "      <td>[dear, local, newspaper,, i, have, found, that...</td>\n",
              "      <td>[dear local, local newspaper,, newspaper, i, i...</td>\n",
              "      <td>[dear local newspaper,, local newspaper, i, ne...</td>\n",
              "      <td>[dear local newspaper, i, local newspaper, i h...</td>\n",
              "      <td>[dear local newspaper, i have, local newspaper...</td>\n",
              "      <td>60</td>\n",
              "      <td>{after, and, that is, suddenly, when, as, for,...</td>\n",
              "      <td>32.408323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>30</td>\n",
              "      <td>dear , i know having computers has a positive ...</td>\n",
              "      <td>[dear, ,, i, know, having, computers, has, a, ...</td>\n",
              "      <td>[dear ,, , i, i know, know having, having comp...</td>\n",
              "      <td>[dear , i, , i know, i know having, know havin...</td>\n",
              "      <td>[dear , i know, , i know having, i know having...</td>\n",
              "      <td>[dear , i know having, , i know having compute...</td>\n",
              "      <td>58</td>\n",
              "      <td>{between, and, so that, as a result, when, as,...</td>\n",
              "      <td>27.824804</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-914659e4-13c4-41ba-83e5-a48610aea57c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-914659e4-13c4-41ba-83e5-a48610aea57c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-914659e4-13c4-41ba-83e5-a48610aea57c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_connectors['Connectivity_Index'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxVdOwi34ruZ",
        "outputId": "9096473f-a366-4390-c9d0-e046ad2a3c02"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1783.000000\n",
              "mean       22.162848\n",
              "std        10.921233\n",
              "min         0.846574\n",
              "25%        14.385855\n",
              "50%        20.134345\n",
              "75%        27.894780\n",
              "max        95.304019\n",
              "Name: Connectivity_Index, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_connectors['Connectivity_Index'].hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "2ozErOR07J9C",
        "outputId": "ccd62d81-f1b1-498c-a0f1-b916dc1a8dd4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4bb3aeac10>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Readibility Scores**\n",
        "\n",
        "We section will cover:\n",
        "1.   Flesch Reading Ease\n",
        "2.   Flesch-Kincaid Grade Level\n",
        "3.   Gunning Fog Index\n",
        "4.   Dale Chall Readability Formula\n",
        "5.   Shannon Entropy\n",
        "6.   Simpson's Index"
      ],
      "metadata": {
        "id": "-uvdm3joSeUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Flesch_Reading_East_Score(scount,NoOfsentences,total_Words):\n",
        " return (206.835-1.015*(total_Words/float(NoOfsentences))-84.6*(scount / float(total_Words)))"
      ],
      "metadata": {
        "id": "YQm0D0h4Se1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Flesch_Reading_Score\"] = [0] * 1783\n",
        "for index, row in Data_Essay_01.iterrows():\n",
        "  Data_Essay_01['Flesch_Reading_Score'][index] = Flesch_Reading_East_Score(row[\"Syllable_Count\"],row[\"Sent_Count\"],row[\"Word_Count\"])"
      ],
      "metadata": {
        "id": "JxZIddenStzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Flesch_Reading_Score\"]"
      ],
      "metadata": {
        "id": "JgyddvVu5kum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Features Correlation Visualization**"
      ],
      "metadata": {
        "id": "H9_v794WviV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01_Features = pd.read_csv(\"/content/drive/MyDrive/IntelliTech-DataSet/EssaySet01_Features.csv\")\n",
        "Data_Essay_01_Features.head()"
      ],
      "metadata": {
        "id": "0mCatJ5XvoFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01_Features[\"Target_Score\"] = Data_Essay_01[\"Total Score\"]\n",
        "Data_Essay_01_Features[\"Flesch_Reading_Score\"] = Data_Essay_01[\"Flesch_Reading_Score\"]\n",
        "Data_Essay_01_Features.columns"
      ],
      "metadata": {
        "id": "KvdRCWsWw0vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPhsV-Ws5xSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01_Features.drop('Unnamed: 0' , axis = 1 , inplace = True)\n",
        "Data_Essay_01_Features.drop(\"Unnamed: 20\" , axis = 1, inplace = True)\n",
        "Data_Essay_01_Features.drop(\"Unnamed: 18\" , axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "Ruper2IjxADt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01_Features.corr()"
      ],
      "metadata": {
        "id": "5UcqnTwq2KPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Plot correlation of essay-length related features\n",
        "cols = ['Sent_Count', 'Word_Count', 'Char_Count']\n",
        "#        'Avg_Word_Count',\n",
        "#        'Verb_Count', 'Noun_Count', 'Adj_Count', 'Conj_Count', 'Adverb_Count',\n",
        "#        'pNoun_Count', 'Count_Fullstops', 'Count_Exclamation', 'Count_Comma',\n",
        "#        'Count_Questionmark', 'Count_Hyphens', 'Capitalization_Errors',\n",
        "#        'Grammar_Error_Count', 'Punctuation_Errors']\n",
        "        \n",
        "g = sns.pairplot(Data_Essay_01_Features, hue='Target_Score', vars=cols, plot_kws={\"s\": 20}, palette=\"bright\")\n",
        "g.fig.subplots_adjust(top=.93)\n",
        "g.fig.suptitle('Pairplots of select features', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GWo9XpQZwfgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}