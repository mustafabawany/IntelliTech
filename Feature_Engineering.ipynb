{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Packages**"
      ],
      "metadata": {
        "id": "t5wewbNqz8cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpP642wc8kXg",
        "outputId": "fb6ab9b7-b38b-4b91-c98b-4b194cde19f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n",
        "!pip install sentencepiece  \n",
        "!pip install transformers\n",
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart Runtime after installing "
      ],
      "metadata": {
        "id": "oFW9msHtUg-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Packages**"
      ],
      "metadata": {
        "id": "-CLigcFJ0ABg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzNPxq8X8bYG",
        "outputId": "99cda787-1350-4f04-e04d-c60f52426533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk \n",
        "import spacy\n",
        "import textstat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn \n",
        "from textblob import Word\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading Data From Google Drive**"
      ],
      "metadata": {
        "id": "B9QmvZ2KGsPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7JhmdSg8swA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "Data_Essay_01 = pd.read_csv(\"/content/drive/MyDrive/IntelliTech-DataSet/EssaySet01.csv\")\n",
        "Data_Essay_01.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4CyChIa3hm3"
      },
      "source": [
        "# **Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Essay Pre Processing**"
      ],
      "metadata": {
        "id": "0d9vjiulC7aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Remove_NER(Essay):\n",
        "  \"\"\"\n",
        "    Removes Named Entity Recognition (NER) from each essay\n",
        "\n",
        "    Args:\n",
        "      Sentence: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "\n",
        "  \"\"\"\n",
        "  return ' '.join (word for word in Essay.split(' ') if not word.startswith('@'))\n",
        "\n",
        "def Remove_Punctuations(sentence):\n",
        "  \"\"\"\n",
        "    Removes punctuations from text\n",
        "    Args:\n",
        "      sentence: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  punctuations = '''!()-[]{};:\"\\,/'<>.?@#$%^&*_~'''\n",
        "  newSentence = \"\"\n",
        "  for word in sentence:\n",
        "      if (word in punctuations):\n",
        "          newSentence = newSentence + \" \"\n",
        "      else: \n",
        "          newSentence = newSentence + word\n",
        "  return newSentence\n",
        "\n",
        "def LowerCase_Words(Essay):\n",
        "  \"\"\"\n",
        "    Lower case all the words in an essay\n",
        "\n",
        "    Args:\n",
        "      Sentence: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  return re.sub('[0-9]+','', Essay).lower() \n",
        "\n",
        "def Tokenize_Essay(Essay):\n",
        "    \"\"\"\n",
        "      Create Tokens of each Essay\n",
        "\n",
        "      Args:\n",
        "        Essay: Essay of each student\n",
        "      \n",
        "      Returns: \n",
        "        String\n",
        "    \"\"\"\n",
        "    Preprocessed = Remove_Punctuations(Essay)\n",
        "    return \" \".join(word_tokenize(Preprocessed))\n",
        "\n",
        "def Remove_White_Spaces(Essay):\n",
        "  \"\"\"\n",
        "    Removes Extra White Spaces\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  return \" \".join(Essay.split())\n",
        "\n",
        "def Remove_Special_Characters(Essay):\n",
        "  \"\"\"\n",
        "    Removes Special Characters from Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student\n",
        "    \n",
        "    Returns: \n",
        "      String\n",
        "  \"\"\"\n",
        "  new_text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", Essay)\n",
        "  return new_text"
      ],
      "metadata": {
        "id": "6qLIKn4LC7-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Basic Count Features**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Sentences per Essay\n",
        "*   Counting Words per Essay\n",
        "*   Counting Characters per Essay\n",
        "*   Average Words per Essay\n",
        "*   Counting Syllables\n"
      ],
      "metadata": {
        "id": "pmuQWvabiYC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Sentences per Essay"
      ],
      "metadata": {
        "id": "9jYVbje0Ex_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV3ttAkblI57"
      },
      "outputs": [],
      "source": [
        "def Sentence_Count(Essay):\n",
        "    \"\"\"\n",
        "    Counts sentences in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "    sentence_no = nltk.sent_tokenize(Essay)\n",
        "    return len(sentence_no)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Sent_Count'] = Data_Essay_01['Essay'].apply(Sentence_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "hQzk8MtddUbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Words per Essay"
      ],
      "metadata": {
        "id": "6VY92aM5E00E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** These word count are more than the original count coz of nltk tokenization. Punctations are treated as seperate words.\n"
      ],
      "metadata": {
        "id": "YxGrgzWIFzPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Word_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts words in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int  \n",
        "  \"\"\" \n",
        "  word_no = nltk.word_tokenize(Essay)\n",
        "  return len(word_no)"
      ],
      "metadata": {
        "id": "7OPVrqpAdLec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Word_Count'] = Data_Essay_01['Essay'].apply(Word_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "8kb3-p5edQzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Characters per Essay"
      ],
      "metadata": {
        "id": "LCGMHv_yE29j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Char_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts characters in an essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "  return len([character for character in Essay])"
      ],
      "metadata": {
        "id": "ELmS3Tt-f-nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Char_Count'] = Data_Essay_01['Essay'].apply(Char_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "AchO9qj9dMzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Average Word Length of Essay"
      ],
      "metadata": {
        "id": "eSWTBDDpizP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Avg_Word_Count(Essay):\n",
        "  \"\"\"\n",
        "    Calculates Average Word Count In An Essay Set\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      float\n",
        "      \n",
        "  \"\"\"\n",
        "  word_list = nltk.word_tokenize(Essay)\n",
        "  total = sum(map(len, word_list))/len(word_list)\n",
        "  return total"
      ],
      "metadata": {
        "id": "HujBMTSuXxxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Avg_Word_Count'] = Data_Essay_01['Essay'].apply(Avg_Word_Count)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "D5A2-UkIdX4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Syllables"
      ],
      "metadata": {
        "id": "SSNhc_FfKHld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Syllable_Count(text):\n",
        "  return textstat.syllable_count(text, lang='en_US')"
      ],
      "metadata": {
        "id": "1ihBnNT6KGnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parts Of Speech Counts**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Nouns per Essay\n",
        "*   Counting Adjectives per Essay\n",
        "*   Counting Proper Nouns per Essay\n",
        "*   Counting Adverbs per Essay\n",
        "*   Counting Conjunctions per Essay"
      ],
      "metadata": {
        "id": "iUMnW4Qm4Vws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing NERs, Punctuations and Lower Casing"
      ],
      "metadata": {
        "id": "A0qjHkwRH7FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Preprocessed_Essay'] = Data_Essay_01['Essay'].apply(Remove_NER)\n",
        "Data_Essay_01['Preprocessed_Essay'] = Data_Essay_01['Preprocessed_Essay'].apply(Tokenize_Essay)\n",
        "Data_Essay_01.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "mxsK-yprDcfV",
        "outputId": "bdc14352-785a-470d-e5f5-a8b86c754dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                              Essay  Rater_1 Score  \\\n",
              "0   1  Dear local newspaper, I think effects computer...            4.0   \n",
              "1   2  Dear @CAPS1 @CAPS2, I believe that using compu...            5.0   \n",
              "2   3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...            4.0   \n",
              "3   4  Dear Local Newspaper, @CAPS1 I have found that...            5.0   \n",
              "4   5  Dear @LOCATION1, I know having computers has a...            4.0   \n",
              "\n",
              "   Rater_2 Score  Total Score  \\\n",
              "0            4.0          8.0   \n",
              "1            4.0          9.0   \n",
              "2            3.0          7.0   \n",
              "3            5.0         10.0   \n",
              "4            4.0          8.0   \n",
              "\n",
              "                                  Preprocessed_Essay  Count_Fullstops  \\\n",
              "0  Dear local newspaper I think effects computers...               10   \n",
              "1  Dear I believe that using computers will benef...               18   \n",
              "2  Dear More and more people use computers but no...               14   \n",
              "3  Dear Local Newspaper I have found that many ex...               24   \n",
              "4  Dear I know having computers has a positive ef...               30   \n",
              "\n",
              "   Count_Exclamation  Count_Comma  Count_Questionmark  Count_Hyphens  \n",
              "0                  4           18                   2              1  \n",
              "1                  1           12                   1              0  \n",
              "2                  0            9                   0              0  \n",
              "3                  2           13                   1              0  \n",
              "4                  0           13                   0              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6461886-6ed2-49f4-bb6f-73d055db952f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Preprocessed_Essay</th>\n",
              "      <th>Count_Fullstops</th>\n",
              "      <th>Count_Exclamation</th>\n",
              "      <th>Count_Comma</th>\n",
              "      <th>Count_Questionmark</th>\n",
              "      <th>Count_Hyphens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>Dear local newspaper I think effects computers...</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Dear I believe that using computers will benef...</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Dear More and more people use computers but no...</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Dear Local Newspaper I have found that many ex...</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>Dear I know having computers has a positive ef...</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6461886-6ed2-49f4-bb6f-73d055db952f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6461886-6ed2-49f4-bb6f-73d055db952f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6461886-6ed2-49f4-bb6f-73d055db952f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Pos_Tag_Count(Essay):\n",
        "  \"\"\"\n",
        "    Counts Parts of Speech in an Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int,int,int,int,int,int    \n",
        "  \"\"\"\n",
        "  tagged_doc = nlp(Essay)\n",
        "\n",
        "  adj_count=0\n",
        "  verb_count=0\n",
        "  noun_count=0\n",
        "  pNoun_count=0\n",
        "  adverb_count=0\n",
        "  conj_count=0\n",
        "\n",
        "  for token in tagged_doc:\n",
        "\n",
        "    if(token.pos_ == 'ADJ'):\n",
        "      adj_count+=1\n",
        "    \n",
        "    elif(token.pos_ =='NOUN'):\n",
        "      noun_count+=1\n",
        "\n",
        "    elif (token.pos_ =='PRON'):\n",
        "      pNoun_count+=1\n",
        "\n",
        "    elif (token.pos_ =='VERB'):\n",
        "      verb_count+=1\n",
        "\n",
        "    elif (token.pos_ =='ADV'):\n",
        "      adverb_count+=1\n",
        "    \n",
        "    elif(token.pos_=='CCONJ'):\n",
        "      conj_count+=1\n",
        "\n",
        "  return verb_count,noun_count, adj_count, conj_count, adverb_count,pNoun_count"
      ],
      "metadata": {
        "id": "26j3wRKD4X54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Verb_Count'], Data_Essay_01['Noun_Count'], Data_Essay_01['Adj_Count'], Data_Essay_01['Conj_Count'], Data_Essay_01['Adverb_Count'], Data_Essay_01['pNoun_Count']=zip(*Data_Essay_01[\"Preprocessed_Essay\"].map(Pos_Tag_Count))\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "W-WSN5Ce85Cu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "30a10385-3704-431b-99cc-2bb12ad9af1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ID                                              Essay  Rater_1 Score  \\\n",
              "1322  1327  Dear @CAPS1, I am writing to express my opinio...            5.0   \n",
              "\n",
              "      Rater_2 Score  Total Score  \\\n",
              "1322            6.0         11.0   \n",
              "\n",
              "                                     Preprocessed_Essay  Count_Fullstops  \\\n",
              "1322  Dear I am writing to express my opinion on com...               28   \n",
              "\n",
              "      Count_Exclamation  Count_Comma  Count_Questionmark  Count_Hyphens  \\\n",
              "1322                  0           39                   2              3   \n",
              "\n",
              "      Verb_Count  Noun_Count  Adj_Count  Conj_Count  Adverb_Count  pNoun_Count  \n",
              "1322          71         105         26          33            54           62  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e53aeefc-9537-4f2d-be57-dcb96e454fd1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Rater_1 Score</th>\n",
              "      <th>Rater_2 Score</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Preprocessed_Essay</th>\n",
              "      <th>Count_Fullstops</th>\n",
              "      <th>Count_Exclamation</th>\n",
              "      <th>Count_Comma</th>\n",
              "      <th>Count_Questionmark</th>\n",
              "      <th>Count_Hyphens</th>\n",
              "      <th>Verb_Count</th>\n",
              "      <th>Noun_Count</th>\n",
              "      <th>Adj_Count</th>\n",
              "      <th>Conj_Count</th>\n",
              "      <th>Adverb_Count</th>\n",
              "      <th>pNoun_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1322</th>\n",
              "      <td>1327</td>\n",
              "      <td>Dear @CAPS1, I am writing to express my opinio...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>Dear I am writing to express my opinion on com...</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>105</td>\n",
              "      <td>26</td>\n",
              "      <td>33</td>\n",
              "      <td>54</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e53aeefc-9537-4f2d-be57-dcb96e454fd1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e53aeefc-9537-4f2d-be57-dcb96e454fd1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e53aeefc-9537-4f2d-be57-dcb96e454fd1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating Writing Attributes**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Style\n",
        "*   Content\n",
        "*   Semantic\n",
        "*   Semantic Coherence & Consistency \n",
        "*   Connectivity\n",
        "*   Readibility Scores\n"
      ],
      "metadata": {
        "id": "puuRk5EHw8jJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Style**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Mechanics\n",
        "*   Grammar\n",
        "*   Lexical Sophistication\n",
        "\n"
      ],
      "metadata": {
        "id": "Id4wN-9AxITZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mechanics**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Counting Spelling Mistakes\n",
        "*   Checking Punctuations\n",
        "*   Counting Punctuations\n",
        "*   Checking Capitalization\n",
        "\n"
      ],
      "metadata": {
        "id": "I6sEA-fCCfmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Spelling Mistakes"
      ],
      "metadata": {
        "id": "2NgCS1PQZaIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW5kZQP-W3CE"
      },
      "outputs": [],
      "source": [
        "def Check_Spelling(Sentence):\n",
        "  \"\"\"\n",
        "    Checks spelling of each word\n",
        "\n",
        "    Args:\n",
        "      word: Words (Tokens) of each essay \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  Sentence = word_tokenize(Sentence)\n",
        "  for word in Sentence:\n",
        "    word = Word(word)\n",
        "  \n",
        "    result = word.spellcheck()\n",
        "\n",
        "    # result [0][0] contains the bool value if the spelling is correct or not\n",
        "    # result [0][1] contains the confidence for the suggest correct spelling\n",
        "\n",
        "    if word != result[0][0]:\n",
        "      if(result[0][1] > 0.95 and not(wordnet.synsets(word)) and not(\"/\" in word)):\n",
        "        print(word , result[0][0])\n",
        "        count = count + 1\n",
        "\n",
        "  return count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Preprocessed_Essay\"] = Data_Essay_01[\"Essay\"].apply(Remove_NER)\n",
        "Data_Essay_01[\"Preprocessed_Essay\"] = Data_Essay_01[\"Preprocessed_Essay\"].apply(Remove_Punctuations)"
      ],
      "metadata": {
        "id": "Lhbvn0kuFGnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Spelling_Mistakes_Count\"]  = Data_Essay_01[\"Preprocessed_Essay\"].map(Check_Spelling)\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "u5UO3OIYE-8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking Punctuation Mistakes **(Incomplete)**"
      ],
      "metadata": {
        "id": "qEPOKuVJRz1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correcting Spelling Mistakes via TextBlob"
      ],
      "metadata": {
        "id": "EKmSFD923yAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Correct_Spelling(Sentence):\n",
        "  \"\"\"\n",
        "    Checks spelling of each word\n",
        "\n",
        "    Args:\n",
        "      word: Words (Tokens) of each essay \n",
        "    \n",
        "    Returns: \n",
        "      int\n",
        "      \n",
        "  \"\"\"\n",
        "  Tokens = word_tokenize(Sentence)\n",
        "  newTokens = []\n",
        "  for word in Tokens:\n",
        "    word = Word(word)\n",
        "  \n",
        "    result = word.spellcheck()\n",
        "\n",
        "    # result [0][0] contains the bool value if the spelling is correct or not\n",
        "    # result [0][1] contains the confidence for the suggest correct spelling\n",
        "    \n",
        "    if word != result[0][0]:\n",
        "      if(result[0][1] > 0.9 and not(wordnet.synsets(word)) and not(\"/\" in word) and not(\"@\" in word)):\n",
        "        newTokens.append(result[0][0])\n",
        "        print(word , result[0][0])\n",
        "      else: \n",
        "        newTokens.append(word)\n",
        "    else:\n",
        "      newTokens.append(word)\n",
        "  return ' '.join(newTokens)"
      ],
      "metadata": {
        "id": "6BCrmEOO3vfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Essay_Corrected'] = Data_Essay_01['Essay'].apply(Remove_White_Spaces)"
      ],
      "metadata": {
        "id": "P189o66CU391"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Essay_Corrected\"]  = Data_Essay_01[\"Essay_Corrected\"].map(Correct_Spelling)"
      ],
      "metadata": {
        "id": "sa2lPNPmU6Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correcting Spelling Mistakes via LanguageTool"
      ],
      "metadata": {
        "id": "ndVmwOgqVIfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Spelling_Error_Correct(essays):\n",
        "    matches = tool.check(essays)\n",
        "    is_bad_rule = lambda rule: rule.category == 'GRAMMAR'\n",
        "    matches = [rule for rule in matches if not is_bad_rule(rule)]\n",
        "    # print(matches[0].category)\n",
        "    language_tool_python.utils.correct(essays, matches)   # to correct it\n",
        "    return essays"
      ],
      "metadata": {
        "id": "dLb4MnckVMVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01['Essay_Correct_LT'] = Data_Essay_01['Essay'].apply(Spelling_Error_Correct)"
      ],
      "metadata": {
        "id": "i3m8RRJJVMMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Punctuation Mistakes"
      ],
      "metadata": {
        "id": "N2BerbAju6gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification , pipeline"
      ],
      "metadata": {
        "id": "9oOhfQJHdo0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('oliverguhr/fullstop-punctuation-multilang-large')\n",
        "model = AutoModelForTokenClassification.from_pretrained('oliverguhr/fullstop-punctuation-multilang-large')\n",
        "pun = pipeline('ner' , model = model , tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "Wa2ndqK-T2p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = pun(text)\n",
        "\n",
        "Updated_string = ''\n",
        "\n",
        "for output in tags:\n",
        "  result = output['word'].replace('▁' , ' ') + output['entity'].replace('0', '')\n",
        "  Updated_string += result\n",
        "\n",
        "Updated_string"
      ],
      "metadata": {
        "id": "BZ4O_fCzUwHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Counting Number Of Punctuations"
      ],
      "metadata": {
        "id": "0GCMQ41VQbyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Count_Punctuations(Essay):\n",
        "  \"\"\"\n",
        "    Counts Punctuations used in an Essay\n",
        "\n",
        "    Args:\n",
        "      Essay: Essay of each student \n",
        "    \n",
        "    Returns: \n",
        "      int,int,int,int,int\n",
        "      \n",
        "  \"\"\"\n",
        "  count_fullstops = 0\n",
        "  count_exclamation = 0\n",
        "  count_comma = 0\n",
        "  count_hyphens = 0\n",
        "  count_questionmark = 0\n",
        "\n",
        "  tokens = word_tokenize(Essay)\n",
        "\n",
        "  for word in tokens:\n",
        "    if word == \".\":\n",
        "      count_fullstops += 1\n",
        "    elif word == \"!\":\n",
        "      count_exclamation += 1\n",
        "    elif word == \"?\":\n",
        "      count_questionmark += 1\n",
        "    elif word == \",\":\n",
        "      count_comma += 1\n",
        "    elif word == \"-\":\n",
        "      count_hyphens += 1\n",
        "\n",
        "  return count_fullstops , count_exclamation , count_comma , count_questionmark , count_hyphens"
      ],
      "metadata": {
        "id": "c8GDruwFQbYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01[\"Count_Fullstops\"] , Data_Essay_01[\"Count_Exclamation\"] , Data_Essay_01[\"Count_Comma\"] , Data_Essay_01[\"Count_Questionmark\"] , Data_Essay_01[\"Count_Hyphens\"] = zip(*Data_Essay_01[\"Essay\"].map(Count_Punctuations))\n",
        "Data_Essay_01.sample()"
      ],
      "metadata": {
        "id": "5ZTt9va_Vsuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking Capitalization Mistakes"
      ],
      "metadata": {
        "id": "3mZ9RfrmQwpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Check_Captialization(Essay):\n",
        "  \"\"\"\n",
        "    Checks capitalization in each sentence of an essay\n",
        "\n",
        "    Args:\n",
        "    Essay: Words (Tokens) of each essay \n",
        "\n",
        "    Returns: \n",
        "    int\n",
        "\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "\n",
        "  words = word_tokenize(Essay)\n",
        "\n",
        "  # Checking Capital Letters in start of every sentence & start of every quote\n",
        "  for i in range(len(words) - 1):\n",
        "    \n",
        "    if words[i] == '.' or words[i] == '\"':\n",
        "        match = words[i+1]\n",
        "        if match != words[i+1].title():\n",
        "          count = count + 1\n",
        "\n",
        "  # Checking if all proper nouns are capital or not\n",
        "  tagged_sent = pos_tag(words)\n",
        "\n",
        "  for word,pos in tagged_sent:\n",
        "    if(pos == 'NNP'):\n",
        "      if word != word.title():\n",
        "        count = count + 1\n",
        "\n",
        "  return count"
      ],
      "metadata": {
        "id": "IP6L-cA7SArH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Check_Captialization(Remove_NER(Data_Essay_01[\"Essay\"][9]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOma5U8XSBBn",
        "outputId": "679ae0a9-0943-462d-afaa-a295dbfeea0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Remove_NER(Data_Essay_01[\"Essay\"][12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "0K9iZpWFSBTY",
        "outputId": "bb62c89d-8f83-4466-f027-76f97b900849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dear local newspaper i raed ur argument on the computers and i think they are a positive effect on people. the first reson i think they are a good effect is because you can do so much with them like if you live in mane and ur cuzin lives in califan you and him could have a wed chat. the second thing you could do is look up news any were in the world you could be stuck on a plane and it would be vary boring when you can take but ur computer and go on ur computer at work and start doing work. when you said it takes away from exirsis well some people use the computer for that too to chart how fast they run or how meny miles they want and sometimes what they eat. the thrid reson is some peolpe jobs are on the computers or making computers for exmple when you made this artical you didnt use a type writer you used a computer and printed it out if we didnt have computers it would make ur a lot harder. thank you for reading and whe you are thinking adout it agen pleas consiter my thrie resons.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Grammar Error Detection**"
      ],
      "metadata": {
        "id": "kZEkV3-ruDBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# reference = result.text.split()\n",
        "\n",
        "# candidate = 'Dear local newspaper, @CAPS1 best friend, @LOCATION2, was once a nerd with no hand-eye coordination, @CAPS2, he started to use a computer and now he has better hand-eye coordination than me.'.split()\n",
        "# print('BLEU score -> {}'.format(sentence_bleu(reference, candidate )))"
      ],
      "metadata": {
        "id": "dxk_61AvuGQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = Data_Essay_01[['Essay', 'Sent_Count']]\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "x2Y4N0k4uLKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grammar Error via CFG\n",
        "\n",
        "# def grammar_error(essays_1,sent_count):\n",
        "#     sentences = nltk.sent_tokenize(essays_1[1])\n",
        "#     for sent in range(0,sent_count):\n",
        "#        wrong =1\n",
        "#        sent_split = sentences[sent].split()  \n",
        "#        tagged = nltk.pos_tag(sent_split) \n",
        "#        tags = [x[1].lower() for x in tagged] \n",
        "\n",
        "#        try:\n",
        "#         parser = nltk.RecursiveDescentParser(grammar)\n",
        "        \n",
        "#         for tree in parser.parse(tags):\n",
        "#             s = tree\n",
        "#             wrong =0\n",
        "#             print(\"Correct Grammar!!!!\")\n",
        "#             print(\"*\"*20)\n",
        "        \n",
        "#         if wrong ==1:\n",
        "#             print(\"Wrong Grammar!!!\")\n",
        "#             print(\"*\"*20)\n",
        "    \n",
        "#        except ValueError:\n",
        "#         print(\"Sorry! Some words are not covered in the grammar yet :)\")\n",
        "\n",
        "    \n",
        "# essays_1 = df1['Essay_Clean'].tolist()\n",
        "# sent_count = df1['Clean_Sent_Count'].tolist()\n",
        "# grammar_error(essays_1,sent_count[1])"
      ],
      "metadata": {
        "id": "XNwkbsDFuOWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import language_tool_python\n",
        "tool = language_tool_python.LanguageTool('en-US')"
      ],
      "metadata": {
        "id": "6nYGovM1uPu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = Data_Essay_01[['Essay', 'Sent_Count']]\n",
        "df1['Essay'] = df1['Essay'].apply(Remove_White_Spaces)   # to avoid whitespace error\n",
        "df1['Essay']"
      ],
      "metadata": {
        "id": "3CrujkhOuQ_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Grammar_Errors(essays):\n",
        "    \n",
        "    matches = tool.check(essays)\n",
        "    is_bad_rule = lambda rule: rule.category == 'GRAMMAR'\n",
        "    matches = [rule for rule in matches if is_bad_rule(rule)]\n",
        "    # print(matches[0].category)\n",
        "    errors = []\n",
        "    #language_tool_python.utils.correct(text, matches)   # to correct it\n",
        "    for i in range(0, len(matches)):\n",
        "      errors.append(matches[i].ruleId)  # or category of the error (Misc, Whitespace, Typography)\n",
        "    return len(matches), errors"
      ],
      "metadata": {
        "id": "2kdFSdAfuSWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Grammar_Errors(\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\")"
      ],
      "metadata": {
        "id": "VEgaOfy5WE9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data_Essay_01['Grammar_Errors'], Data_Essay_01['Grammar_Error_List'] = zip(*df1_copy['Essay'].map(grammar_errors))\n",
        "Data_Essay_01['Grammar_Errors'], Data_Essay_01['Grammar_Error_List'] = zip(*df1['Essay'].map(Grammar_Errors))"
      ],
      "metadata": {
        "id": "UCNsXW7MuToX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Essay_01.head(20)"
      ],
      "metadata": {
        "id": "qby9k_DduUsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = Data_Essay_01['Grammar_Error_List'].explode().value_counts()\n",
        "out"
      ],
      "metadata": {
        "id": "4KttcpfcuV6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.to_csv('GrammarErrors.csv')"
      ],
      "metadata": {
        "id": "Mj7LNN5JuW2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lexical Sophistication**"
      ],
      "metadata": {
        "id": "ttyjcisFxNHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install taaled\n",
        "#RESOURCES FOR LEXICAL SOPHISTICATION \n",
        "#https://eli-data-mining-group.github.io/Pitt-ELI-Corpus/publications/Naismith_2019.pdf\n",
        "#https://pypi.org/project/taaled/\n",
        "#https://github.com/LCR-ADS-Lab/pylats"
      ],
      "metadata": {
        "id": "PT5e5b5axQSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVX_WRUzxQNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LL9lGScHxQFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Content**\n",
        "\n",
        "This section will cover:\n",
        "\n",
        "\n",
        "*   Latent Semantic Analysis (LSA)\n"
      ],
      "metadata": {
        "id": "iNcrlo78xaLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Latent Semantic Analysis (LSA)**\n",
        "\n",
        "Content analysis generally implies only a high-level semantic analysis and comparison with source text and graded essays"
      ],
      "metadata": {
        "id": "0Mv-IDjowJET"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gs2o1P6xesp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantic**\n",
        "Semantic metrics assess the correctness of content connotation"
      ],
      "metadata": {
        "id": "MSiuF2JzyBWX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cExW4ZuiyWyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantic Coherence & Consistency**"
      ],
      "metadata": {
        "id": "jQ--G0SrwJqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2-_rC8mwdfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Connectivity**"
      ],
      "metadata": {
        "id": "nEFmhVcBwKAq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYamIfnRwuqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Readibility Scores**\n",
        "\n",
        "We section will cover:\n",
        "1.   Flesch Reading Ease\n",
        "2.   Flesch-Kincaid Grade Level\n",
        "3.   Gunning Fog Index\n",
        "4.   Dale Chall Readability Formula\n",
        "5.   Shannon Entropy\n",
        "6.   Simpson's Index"
      ],
      "metadata": {
        "id": "-uvdm3joSeUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Flesch_Reading_East_Score(scount,NoOfsentences,total_Words):\n",
        " return (206.835-1.015*(total_Words/float(NoOfsentences))-84.6*(scount / float(total_Words)))"
      ],
      "metadata": {
        "id": "YQm0D0h4Se1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in Data_Essay_01.iterrows():\n",
        "  Data_Essay_01['Flesch_Reading_Score'][index]=Flesch_Reading_East_Score(row[\"Syllable_Count\"],row[\"Sent_Count\"],row[\"Word_Count\"])"
      ],
      "metadata": {
        "id": "JxZIddenStzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0d9vjiulC7aL",
        "pmuQWvabiYC-",
        "9jYVbje0Ex_m",
        "6VY92aM5E00E",
        "LCGMHv_yE29j",
        "eSWTBDDpizP2",
        "SSNhc_FfKHld",
        "iUMnW4Qm4Vws",
        "I6sEA-fCCfmN",
        "qEPOKuVJRz1-",
        "0GCMQ41VQbyC",
        "3mZ9RfrmQwpQ",
        "kZEkV3-ruDBc",
        "ttyjcisFxNHZ",
        "iNcrlo78xaLJ",
        "0Mv-IDjowJET",
        "MSiuF2JzyBWX",
        "jQ--G0SrwJqQ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}